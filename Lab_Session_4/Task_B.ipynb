{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49858881",
   "metadata": {},
   "source": [
    "# Lab Session 4 - Task B: Word2Vec\n",
    "\n",
    "1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b5c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Data Loading and Preprocessing (from Task A) \n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "def load_data(folder_path):\n",
    "    all_docs = []\n",
    "    for category in os.listdir(folder_path):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "        if not os.path.isdir(category_path): continue\n",
    "        for filename in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                all_docs.append(preprocess_text(f.read()))\n",
    "    return all_docs\n",
    "\n",
    "BBC_FOLDER = 'bbc'\n",
    "documents = load_data(BBC_FOLDER)\n",
    "\n",
    "# Vocabulary Creation (from Task A) \n",
    "VOCAB_SIZE = 10000\n",
    "all_tokens = [token for doc in documents for token in doc]\n",
    "word_counts = Counter(all_tokens)\n",
    "vocab = [word for word, count in word_counts.most_common(VOCAB_SIZE)]\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "print(f\"Setup complete. Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd048bbf",
   "metadata": {},
   "source": [
    " 2. Prepare Data for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1b6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating training pairs: 100%|██████████| 2225/2225 [00:01<00:00, 1968.31it/s]\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "NUM_NEGATIVE_SAMPLES = 5\n",
    "\n",
    "# Generate skip-gram pairs\n",
    "pairs = []\n",
    "for doc in tqdm(documents, desc=\"Generating training pairs\"):\n",
    "    doc_indices = [word_to_id[word] for word in doc if word in vocab_set]\n",
    "    for i, target_idx in enumerate(doc_indices):\n",
    "        start = max(0, i - WINDOW_SIZE)\n",
    "        end = min(len(doc_indices), i + WINDOW_SIZE + 1)\n",
    "        context_indices = doc_indices[start:i] + doc_indices[i+1:end]\n",
    "        for context_idx in context_indices:\n",
    "            pairs.append((target_idx, context_idx))\n",
    "\n",
    "# Prepare for negative sampling\n",
    "word_freqs = np.array([word_counts[word] for word in vocab])\n",
    "unigram_dist = word_freqs**0.75 / np.sum(word_freqs**0.75)\n",
    "word_indices = np.arange(VOCAB_SIZE)\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, unigram_dist, num_negative_samples):\n",
    "        self.pairs = pairs\n",
    "        self.unigram_dist = unigram_dist\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        negative_samples = np.random.choice(word_indices, size=self.num_negative_samples, p=self.unigram_dist)\n",
    "        return torch.LongTensor([target]), torch.LongTensor([context]), torch.LongTensor(negative_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148a819",
   "metadata": {},
   "source": [
    "3. Define the PyTorch Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "388465d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.init_embed()\n",
    "\n",
    "    def init_embed(self):\n",
    "        nn.init.uniform_(self.in_embed.weight, -0.5 / self.in_embed.embedding_dim, 0.5 / self.in_embed.embedding_dim)\n",
    "        nn.init.uniform_(self.out_embed.weight, -0.5 / self.out_embed.embedding_dim, 0.5 / self.out_embed.embedding_dim)\n",
    "        \n",
    "    def forward(self, target_word, context_word, negative_words):\n",
    "        v_target = self.in_embed(target_word)  \n",
    "        v_context = self.out_embed(context_word) \n",
    "        v_negs = self.out_embed(negative_words)\n",
    "        \n",
    "        pos_score = torch.bmm(v_target, v_context.transpose(1, 2)).squeeze(2)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-8).mean()\n",
    "        \n",
    "        neg_score = torch.bmm(v_target.expand(v_negs.size()), v_negs.transpose(1, 2))\n",
    "        neg_loss = -torch.log(torch.sigmoid(-neg_score) + 1e-8).mean()\n",
    "        \n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c2b3b",
   "metadata": {},
   "source": [
    "4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 4139/4139 [13:10<00:00,  5.24it/s, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.2633462648053018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 4139/4139 [12:47<00:00,  5.40it/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 1.1624225196875244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 4139/4139 [13:21<00:00,  5.16it/s, loss=1.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 1.1048398629050522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset = Word2VecDataset(pairs, unigram_dist, NUM_NEGATIVE_SAMPLES)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = SkipGramNegativeSampling(VOCAB_SIZE, EMBEDDING_DIM).to(device)\n",
    "optimizer = optim.SparseAdam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for target, context, negs in pbar:\n",
    "        target, context, negs = target.to(device), context.to(device), negs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target, context, negs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "word2vec_embeddings = model.in_embed.weight.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444489de",
   "metadata": {},
   "source": [
    "5. Task B.a: Word Similarity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f4786a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Word2Vec Similarity Results ---\n",
      "market: stock, ntpc, upsurge, fujitsu, steadily\n",
      "film: berlin, directed, fahrenheit, daylewis, gibsons\n",
      "election: campaigning, looming, slogan, turnout, polls\n",
      "game: encounter, warcraft, toshack, kicker, awesome\n",
      "software: opensource, programs, patents, linux, windows\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(query_word, matrix, word_to_id, id_to_word, top_n=5):\n",
    "    if query_word not in word_to_id:\n",
    "        return [(\"Word not in vocabulary\", 0)] * top_n\n",
    "    query_id = word_to_id[query_word]\n",
    "    query_vector = matrix[query_id].reshape(1, -1)\n",
    "    sim_scores = cosine_similarity(query_vector, matrix).flatten()\n",
    "    top_indices = np.argsort(sim_scores)[::-1][1:top_n+1]\n",
    "    return [(id_to_word[i], sim_scores[i]) for i in top_indices]\n",
    "\n",
    "query_words = ['market', 'film', 'election', 'game', 'software']\n",
    "word2vec_results = []\n",
    "\n",
    "for word in query_words:\n",
    "    similar_words = find_most_similar(word, word2vec_embeddings, word_to_id, id_to_word)\n",
    "    word2vec_results.append(\", \".join([w[0] for w in similar_words]))\n",
    "\n",
    "print(\"\\n--- Word2Vec Similarity Results ---\")\n",
    "for q, res in zip(query_words, word2vec_results):\n",
    "    print(f\"{q}: {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf12f81",
   "metadata": {},
   "source": [
    "6. Task B.b: Analogy Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8fee78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Analogy Question Results \n",
      "\n",
      "business is to profit as politics is to ?\n",
      "  -> Answer: cardinal\n",
      "\n",
      "britain is to london as france is to ?\n",
      "  -> Answer: germany\n",
      "\n",
      "sport is to football as tech is to ?\n",
      "  -> Answer: exploiting\n",
      "\n",
      "minister is to government as player is to ?\n",
      "  -> Answer: prime\n",
      "\n",
      "movie is to entertainment as computer is to ?\n",
      "  -> Answer: mouse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy(a, b, c, matrix, word_to_id, id_to_word):\n",
    "    for word in [a, b, c]:\n",
    "        if word not in word_to_id:\n",
    "            return f\"Error: '{word}' not in vocabulary.\"\n",
    "    vec_a = matrix[word_to_id[a]]\n",
    "    vec_b = matrix[word_to_id[b]]\n",
    "    vec_c = matrix[word_to_id[c]]\n",
    "    result_vec = (vec_a - vec_b + vec_c).reshape(1, -1)\n",
    "    sim_scores = cosine_similarity(result_vec, matrix).flatten()\n",
    "    top_indices = np.argsort(sim_scores)[::-1]\n",
    "    for idx in top_indices:\n",
    "        word = id_to_word[idx]\n",
    "        if word not in [a, b, c]:\n",
    "            return word\n",
    "    return \"No answer found.\"\n",
    "\n",
    "analogies = [\n",
    "    (\"business is to profit as politics is to ?\", ('business', 'profit', 'politics')),\n",
    "    (\"britain is to london as france is to ?\", ('britain', 'london', 'france')),\n",
    "    (\"sport is to football as tech is to ?\", ('sport', 'football', 'tech')),\n",
    "    (\"minister is to government as player is to ?\", ('minister', 'government', 'player')),\n",
    "    (\"movie is to entertainment as computer is to ?\", ('movie', 'entertainment', 'computer'))\n",
    "]\n",
    "\n",
    "print(\"\\n Analogy Question Results \\n\")\n",
    "for question_text, (a, b, c) in analogies:\n",
    "    answer = solve_analogy(a, b, c, word2vec_embeddings, word_to_id, id_to_word)\n",
    "    print(f\"{question_text}\\n  -> Answer: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
