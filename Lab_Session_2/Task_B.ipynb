{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d86d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (2.14.1)\n",
      "Requirement already satisfied: contractions in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be53389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import demoji\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.data.path.append(\"C:/nltk_data\")\n",
    "try:\n",
    "    nltk.data.find('corpora/brown')\n",
    "except LookupError:\n",
    "    nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c2631",
   "metadata": {},
   "source": [
    "Part A: Apply the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68db08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        demoji.download_codes()\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def handle_emojis(self, text):\n",
    "        return demoji.replace(text, \" \")\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\b\\d{4}[-/]\\d{2}[-/]\\d{2}\\b|\\b\\d{2}[-/]\\d{2}[-/]\\d{4}\\b', ' DATE ', text)\n",
    "        text = re.sub(r'\\b\\d+[\\d,.]*\\b', ' NUM ', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def process(self, text):\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.handle_emojis(text)\n",
    "        text = self.normalize_text(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e0ca3",
   "metadata": {},
   "source": [
    "Part B: Training the n-gram model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05952652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, n=3):\n",
    "        self.n = n\n",
    "        self.counts = {i: defaultdict(Counter) for i in range(1, n + 1)}\n",
    "        self.lambdas = np.ones(n) / n\n",
    "        self.vocab = set()\n",
    "        self.total_words = 0\n",
    "\n",
    "    def train(self, sentences, held_out_sentences, em_iterations=10):\n",
    "        print(\"Starting training...\")\n",
    "        padded_sentences = []\n",
    "        for sentence in sentences:\n",
    "            padded_sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "            for token in padded_sentence:\n",
    "                self.vocab.add(token)\n",
    "        self.total_words = sum(len(s) for s in padded_sentences)\n",
    "        for sentence in padded_sentences:\n",
    "            for i in range(1, self.n + 1):\n",
    "                for gram in ngrams(sentence, i):\n",
    "                    if i == 1: \n",
    "                        self.counts[1][()][gram[0]] += 1\n",
    "                    else: \n",
    "                        self.counts[i][gram[:-1]][gram[-1]] += 1\n",
    "        print(f\"Training complete. Vocabulary size: {len(self.vocab)}\")\n",
    "        print(\"Learning interpolation weights using EM algorithm...\")\n",
    "        self.learn_lambdas_em(held_out_sentences, em_iterations)\n",
    "\n",
    "    def get_ngram_prob(self, word, context, order):\n",
    "        if order == 1:\n",
    "            return self.counts[1][()][word] / self.total_words\n",
    "        \n",
    "        context_count = sum(self.counts[order][context].values())\n",
    "        if context_count == 0:\n",
    "            return 0\n",
    "        \n",
    "        word_count_in_context = self.counts[order][context].get(word, 0)\n",
    "        return word_count_in_context / context_count\n",
    "\n",
    "    def learn_lambdas_em(self, held_out, iterations):\n",
    "        for i in range(iterations):\n",
    "            expected_counts = np.zeros(self.n)\n",
    "\n",
    "            for sentence in held_out:\n",
    "                padded_sentence = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "                for j in range(self.n - 1, len(padded_sentence)):\n",
    "                    history = tuple(padded_sentence[j - self.n + 1 : j])\n",
    "                    word = padded_sentence[j]\n",
    "                    \n",
    "                    probs = np.zeros(self.n)\n",
    "                    for k in range(self.n):\n",
    "                        order = k + 1\n",
    "                        context = history[-(order-1):] if order > 1 else ()\n",
    "                        probs[k] = self.get_ngram_prob(word, context, order)\n",
    "                    \n",
    "                    total_prob = np.dot(self.lambdas, probs)\n",
    "                    if total_prob > 1e-9:\n",
    "                        for k in range(self.n):\n",
    "                            expected_counts[k] += (self.lambdas[k] * probs[k]) / total_prob\n",
    "            \n",
    "            self.lambdas = expected_counts / np.sum(expected_counts)\n",
    "            print(f\"EM Iteration {i+1}/{iterations}, New Lambdas: {self.lambdas}\")\n",
    "\n",
    "    def get_interpolated_prob(self, word, history):\n",
    "        prob = 0.0\n",
    "        for i in range(self.n):\n",
    "            order = self.n - i\n",
    "            context = tuple(history[-(order-1):]) if order > 1 else ()\n",
    "            mle_prob = self.get_ngram_prob(word, context, order)\n",
    "            prob += self.lambdas[i] * mle_prob\n",
    "        return prob\n",
    "\n",
    "#Part C: Sentence Completion Task\n",
    "\n",
    "    def generate_sentence(self, prefix, max_length=20, k=5):\n",
    "        preprocessor = Preprocessor()\n",
    "        tokens = preprocessor.process(prefix)\n",
    "        sentence = ['<s>'] * (self.n - 1) + tokens\n",
    "            \n",
    "        for _ in range(max_length):\n",
    "            history = sentence[-(self.n - 1):]\n",
    "            probs = {word: self.get_interpolated_prob(word, history) for word in self.vocab}\n",
    "            probs.pop('<s>', None)\n",
    "                \n",
    "            sorted_probs = sorted(probs.items(), key=lambda item: item[1], reverse=True)\n",
    "            top_k_words = [item[0] for item in sorted_probs[:k]]\n",
    "            top_k_probs = [item[1] for item in sorted_probs[:k]]\n",
    "                \n",
    "            prob_sum = sum(top_k_probs)\n",
    "            if prob_sum == 0: \n",
    "                break\n",
    "            renormalized_probs = [p / prob_sum for p in top_k_probs]\n",
    "                \n",
    "            next_word = np.random.choice(top_k_words, p=renormalized_probs)\n",
    "            if next_word == '</s>': \n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "            \n",
    "        return ' '.join(sentence[self.n - 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211d11a",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "306c2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Task B: N-gram Language Model \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar\\AppData\\Local\\Temp\\ipykernel_6816\\3719390906.py:3: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training complete. Vocabulary size: 11950\n",
      "Learning interpolation weights using EM algorithm...\n",
      "EM Iteration 1/5, New Lambdas: [0.60280252 0.31212704 0.08507044]\n",
      "EM Iteration 2/5, New Lambdas: [0.65416869 0.30291201 0.04291931]\n",
      "EM Iteration 3/5, New Lambdas: [0.66667662 0.3049985  0.02832488]\n",
      "EM Iteration 4/5, New Lambdas: [0.66967099 0.30889284 0.02143617]\n",
      "EM Iteration 5/5, New Lambdas: [0.67019348 0.31218089 0.01762563]\n",
      "\n",
      " Sentence Completion Examples \n",
      "Prefix: 'I'm director of IIT Ropar announced that'\n",
      "Completed: 'i am director of iit ropar announced that the united nations member with a few soldiers from the antitrust legislation together in yous'\n",
      "\n",
      "Prefix: 'After the club, they went to'\n",
      "Completed: 'after the club they went to press were NUM NUM and the other of the other side in hudson of sherman'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Task B: N-gram Language Model \")\n",
    "from nltk.corpus import brown\n",
    "corpus = brown.sents()[:5000]\n",
    "preprocessor = Preprocessor()\n",
    "processed_corpus = [preprocessor.process(' '.join(s)) for s in corpus]\n",
    "train_data, held_out_data = processed_corpus[:4500], processed_corpus[4500:]\n",
    "\n",
    "trigram_model = NgramLanguageModel(n=3)\n",
    "trigram_model.train(train_data, held_out_data, em_iterations=5)\n",
    "\n",
    "print(\"\\n Sentence Completion Examples \")\n",
    "prefixes = [\"I'm director of IIT Ropar announced that\", \"After the club, they went to\"]\n",
    "for p in prefixes:\n",
    "    generated_text = trigram_model.generate_sentence(p, max_length=15, k=10)\n",
    "    print(f\"Prefix: '{p}'\\nCompleted: '{generated_text}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd3dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
