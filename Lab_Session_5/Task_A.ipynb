{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cd7864",
   "metadata": {},
   "source": [
    "# Task A: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f685048",
   "metadata": {},
   "source": [
    "Enabling CUDA for inbuilt GPU usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b30e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! PyTorch can see your CUDA-enabled GPU.\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Success! PyTorch can see your CUDA-enabled GPU.\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Installation issue: PyTorch cannot see your GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48fdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a89cd",
   "metadata": {},
   "source": [
    "1. DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7062f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_nltk():\n",
    "    \"\"\"Download necessary NLTK data.\"\"\"\n",
    "    try:\n",
    "        stopwords.words('english')\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "def load_data(folder_path):\n",
    "    \"\"\"Loads and preprocesses all documents from the dataset folder.\"\"\"\n",
    "    all_docs = []\n",
    "    print(f\"Loading data from: {folder_path}\")\n",
    "    for category in os.listdir(folder_path):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "        if not os.path.isdir(category_path): continue\n",
    "        for filename in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                all_docs.append(preprocess_text(f.read()))\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d1ecd",
   "metadata": {},
   "source": [
    "2. PYTORCH WORD2VEC IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76ba5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    \"\"\"Dataset for generating skip-gram pairs for Word2Vec.\"\"\"\n",
    "    def __init__(self, pairs, unigram_dist, num_negative_samples, word_indices):\n",
    "        self.pairs = pairs\n",
    "        self.unigram_dist = unigram_dist\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.word_indices = word_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        negative_samples = np.random.choice(\n",
    "            self.word_indices, size=self.num_negative_samples, p=self.unigram_dist\n",
    "        )\n",
    "        return torch.LongTensor([target]), torch.LongTensor([context]), torch.LongTensor(negative_samples)\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    \"\"\"PyTorch implementation of Skip-Gram model with Negative Sampling.\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "        self.init_embed()\n",
    "\n",
    "    def init_embed(self):\n",
    "        nn.init.uniform_(self.in_embed.weight, -0.5 / self.in_embed.embedding_dim, 0.5 / self.in_embed.embedding_dim)\n",
    "        nn.init.zeros_(self.out_embed.weight)\n",
    "        \n",
    "    def forward(self, target_word, context_word, negative_words):\n",
    "        v_target = self.in_embed(target_word)\n",
    "        v_context = self.out_embed(context_word)\n",
    "        v_negs = self.out_embed(negative_words)\n",
    "        \n",
    "        pos_score = torch.bmm(v_target, v_context.transpose(1, 2)).squeeze(2)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-8).mean()\n",
    "        \n",
    "        neg_score = torch.bmm(v_target.expand(v_negs.size()), v_negs.transpose(1, 2))\n",
    "        neg_loss = -torch.log(torch.sigmoid(-neg_score) + 1e-8).mean()\n",
    "        \n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a2f5a",
   "metadata": {},
   "source": [
    "3. HELPER FUNCTIONS FOR EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b555c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(query_word, matrix, word_to_id, id_to_word, top_n=5):\n",
    "    \"\"\"Finds the top_n most similar words to a query_word.\"\"\"\n",
    "    if query_word not in word_to_id:\n",
    "        return [\"Word not in vocabulary\"] * top_n\n",
    "    query_id = word_to_id[query_word]\n",
    "    query_vector = matrix[query_id].reshape(1, -1)\n",
    "    sim_scores = cosine_similarity(query_vector, matrix).flatten()\n",
    "    top_indices = np.argsort(sim_scores)[::-1][1:top_n+1]\n",
    "    return [id_to_word[i] for i in top_indices]\n",
    "\n",
    "def solve_analogy(a, b, c, matrix, word_to_id, id_to_word):\n",
    "    \"\"\"Solves the analogy 'a is to b as c is to ?'\"\"\"\n",
    "    for word in [a, b, c]:\n",
    "        if word not in word_to_id:\n",
    "            return f\"Error: '{word}' not in vocabulary.\"\n",
    "    \n",
    "    # Vector arithmetic: vec(a) - vec(b) + vec(c)\n",
    "    vec_a = matrix[word_to_id[a]]\n",
    "    vec_b = matrix[word_to_id[b]]\n",
    "    vec_c = matrix[word_to_id[c]]\n",
    "    result_vec = (vec_a - vec_b + vec_c).reshape(1, -1)\n",
    "    \n",
    "    sim_scores = cosine_similarity(result_vec, matrix).flatten()\n",
    "    top_indices = np.argsort(sim_scores)[::-1]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        word = id_to_word[idx]\n",
    "        if word not in [a, b, c]:\n",
    "            return word\n",
    "    return \"No answer found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a0920",
   "metadata": {},
   "source": [
    "4. MAIN EXECUTION (20 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf96215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: bbc\n",
      "Vocabulary size: 10000\n",
      "Generating training pairs for Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:00<00:00, 3012.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/20: 100%|██████████| 4139/4139 [12:47<00:00,  5.39it/s, loss=1.1757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 1.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/20: 100%|██████████| 4139/4139 [12:39<00:00,  5.45it/s, loss=1.1475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 1.1640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/20: 100%|██████████| 4139/4139 [13:12<00:00,  5.22it/s, loss=1.0907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 1.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/20: 100%|██████████| 4139/4139 [12:40<00:00,  5.44it/s, loss=1.0555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 1.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/20: 100%|██████████| 4139/4139 [13:01<00:00,  5.29it/s, loss=1.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 1.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/20: 100%|██████████| 4139/4139 [12:35<00:00,  5.48it/s, loss=0.9689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 0.9852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/20: 100%|██████████| 4139/4139 [12:33<00:00,  5.50it/s, loss=0.9233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/20: 100%|██████████| 4139/4139 [12:37<00:00,  5.47it/s, loss=0.9117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/20: 100%|██████████| 4139/4139 [12:39<00:00,  5.45it/s, loss=0.9041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 0.9056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/20: 100%|██████████| 4139/4139 [12:54<00:00,  5.34it/s, loss=0.8493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 0.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/20: 100%|██████████| 4139/4139 [13:00<00:00,  5.30it/s, loss=0.8749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 0.8682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/20: 100%|██████████| 4139/4139 [12:51<00:00,  5.37it/s, loss=0.8145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 0.8532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/20: 100%|██████████| 4139/4139 [12:36<00:00,  5.47it/s, loss=0.8575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 0.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/20: 100%|██████████| 4139/4139 [12:30<00:00,  5.51it/s, loss=0.8551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 0.8289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/20: 100%|██████████| 4139/4139 [12:41<00:00,  5.44it/s, loss=0.8636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 0.8190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/20: 100%|██████████| 4139/4139 [12:41<00:00,  5.43it/s, loss=0.8551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Average Loss: 0.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/20: 100%|██████████| 4139/4139 [12:36<00:00,  5.47it/s, loss=0.8110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Average Loss: 0.8021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/20: 100%|██████████| 4139/4139 [12:59<00:00,  5.31it/s, loss=0.7813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Average Loss: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/20: 100%|██████████| 4139/4139 [13:04<00:00,  5.28it/s, loss=0.8009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Average Loss: 0.7888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/20: 100%|██████████| 4139/4139 [12:59<00:00,  5.31it/s, loss=0.7962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Average Loss: 0.7825\n",
      "\n",
      "--- Part a: Word Similarity Comparison ---\n",
      "Successfully loaded results from Lab 4 (task_a_results.csv)\n",
      "\n",
      "Final Comparison Table:\n",
      "             Query Word                       Top 5 Similar Words (VSM)                     Top 5 Similar Words (SVD)                      Top 5 Similar Words (Word2Vec)\n",
      "0     market (business)           stock, housing, growth, prices, sales        stock, analysts, share, growth, prices    stock, steadily, capitalisation, wimpey, buoyant\n",
      "1  film (entertainment)          best, awards, actress, director, films           films, movie, awards, best, actress    cheadle, documentary, starring, directed, moores\n",
      "2   election (politics)         general, labour, campaign, blair, party    labour, general, partys, campaign, labours     general, presidential, milburn, slogan, labours\n",
      "3          game (sport)            games, play, players, match, playing          play, games, players, playing, first          encounter, warcraft, kirwan, halo, toshack\n",
      "4       software (tech)  microsoft, programs, users, antivirus, windows  programs, microsoft, windows, users, program  opensource, patents, harbouring, antivirus, patent\n",
      "\n",
      "\n",
      "--- Part b: Analogy Questions ---\n",
      "\n",
      "business is to profit as politics is to ?\n",
      "  -> Answer: distracted\n",
      "\n",
      "britain is to london as france is to ?\n",
      "  -> Answer: germany\n",
      "\n",
      "sport is to football as tech is to ?\n",
      "  -> Answer: shift\n",
      "\n",
      "minister is to government as player is to ?\n",
      "  -> Answer: prime\n",
      "\n",
      "movie is to entertainment as computer is to ?\n",
      "  -> Answer: electrodes\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    setup_nltk()\n",
    "    BBC_FOLDER = 'bbc'\n",
    "    VOCAB_SIZE = 10000\n",
    "    \n",
    "\n",
    "    WINDOW_SIZE = 5         \n",
    "    EMBEDDING_DIM = 300    \n",
    "    NUM_NEGATIVE_SAMPLES = 5\n",
    "    BATCH_SIZE = 1024\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # Data Loading and Vocab Building \n",
    "    documents = load_data(BBC_FOLDER)\n",
    "    all_tokens = [token for doc in documents for token in doc]\n",
    "    word_counts = Counter(all_tokens)\n",
    "    vocab = [word for word, count in word_counts.most_common(VOCAB_SIZE)]\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    vocab_set = set(vocab)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Prepare Data for PyTorch\n",
    "    print(\"Generating training pairs for Word2Vec...\")\n",
    "    pairs = []\n",
    "    for doc in tqdm(documents):\n",
    "        doc_indices = [word_to_id[word] for word in doc if word in vocab_set]\n",
    "        for i, target_idx in enumerate(doc_indices):\n",
    "            start = max(0, i - WINDOW_SIZE)\n",
    "            end = min(len(doc_indices), i + WINDOW_SIZE + 1)\n",
    "            context_indices = doc_indices[start:i] + doc_indices[i+1:end]\n",
    "            for context_idx in context_indices:\n",
    "                pairs.append((target_idx, context_idx))\n",
    "    \n",
    "    word_freqs = np.array([word_counts[word] for word in vocab])\n",
    "    unigram_dist = word_freqs**0.75 / np.sum(word_freqs**0.75)\n",
    "    word_indices = np.arange(VOCAB_SIZE)\n",
    "    \n",
    "    dataset = Word2VecDataset(pairs, unigram_dist, NUM_NEGATIVE_SAMPLES, word_indices)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the Word2Vec Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = SkipGramNegativeSampling(VOCAB_SIZE, EMBEDDING_DIM).to(device)\n",
    "    optimizer = optim.SparseAdam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\")\n",
    "        for target, context, negs in pbar:\n",
    "            target, context, negs = target.to(device), context.to(device), negs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(target, context, negs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    word2vec_embeddings = model.in_embed.weight.cpu().detach().numpy()\n",
    "\n",
    "    #  Part a: Word Similarity Comparison \n",
    "    print(\"\\n--- Part a: Word Similarity Comparison ---\")\n",
    "    \n",
    "    # Load VSM/SVD results from Lab 4\n",
    "    try:\n",
    "        results_df = pd.read_csv('task_a_results.csv')\n",
    "        print(\"Successfully loaded results from Lab 4 (task_a_results.csv)\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: 'task_a_results.csv' not found. Creating a placeholder table.\")\n",
    "        print(\"Please run Step 1 to save the file from your Lab 4 notebook for a full comparison.\")\n",
    "        results_df = pd.DataFrame({\n",
    "            \"Query Word\": [\n",
    "                \"market (business)\", \"film (entertainment)\", \"election (politics)\", \n",
    "                \"game (sport)\", \"software (tech)\"\n",
    "            ],\n",
    "            \"Top 5 Similar Words (VSM)\": [\"...\" for _ in range(5)],\n",
    "            \"Top 5 Similar Words (SVD)\": [\"...\" for _ in range(5)]\n",
    "        })\n",
    "\n",
    "    query_words = ['market', 'film', 'election', 'game', 'software']\n",
    "    w2v_results = []\n",
    "    for word in query_words:\n",
    "        similar_words = find_most_similar(word, word2vec_embeddings, word_to_id, id_to_word)\n",
    "        w2v_results.append(\", \".join(similar_words))\n",
    "    \n",
    "    results_df[\"Top 5 Similar Words (Word2Vec)\"] = w2v_results\n",
    "    \n",
    "    print(\"\\nFinal Comparison Table:\")\n",
    "    print(results_df.to_string())\n",
    "\n",
    "    #  Part b: Analogy Questions \n",
    "    print(\"\\n\\n--- Part b: Analogy Questions ---\")\n",
    "    analogies = [\n",
    "        (\"business is to profit as politics is to ?\", ('business', 'profit', 'politics')),\n",
    "        (\"britain is to london as france is to ?\", ('britain', 'london', 'france')),\n",
    "        (\"sport is to football as tech is to ?\", ('sport', 'football', 'tech')),\n",
    "        (\"minister is to government as player is to ?\", ('minister', 'government', 'player')),\n",
    "        (\"movie is to entertainment as computer is to ?\", ('movie', 'entertainment', 'computer'))\n",
    "    ]\n",
    "\n",
    "    for question_text, (a, b, c) in analogies:\n",
    "        answer = solve_analogy(a, b, c, word2vec_embeddings, word_to_id, id_to_word)\n",
    "        print(f\"\\n{question_text}\")\n",
    "        print(f\"  -> Answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
