{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaaa592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Libraries imported and configuration set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# NLTK for natural language processing tasks\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DATASET_PATH = 'bbc'\n",
    "VOCAB_SIZE = 10000\n",
    "MIN_WORD_COUNT = 5\n",
    "WINDOW_SIZE = 5       # K=5\n",
    "SVD_DIMS = 300        # d=300\n",
    "W2V_DIMS = 300        # d=300\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"\\nLibraries imported and configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428fd29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    texts = []\n",
    "    for category in os.listdir(path):\n",
    "        category_path = os.path.join(path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for filename in os.listdir(category_path):\n",
    "                if filename.endswith('.txt'):\n",
    "                    with open(os.path.join(category_path, filename), 'r', encoding='latin-1') as f:\n",
    "                        texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "def preprocess(texts):\n",
    "    \"\"\"Cleans and tokenizes a list of text documents.\"\"\"\n",
    "    print(\"Preprocessing documents...\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokenized_corpus = []\n",
    "    for doc in texts:\n",
    "        doc = doc.lower()\n",
    "        doc = re.sub(f'[{re.escape(string.punctuation)}]', '', doc)\n",
    "        doc = re.sub(r'\\d+', '', doc)\n",
    "        tokens = word_tokenize(doc)\n",
    "        tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "        tokenized_corpus.append(tokens)\n",
    "    print(\"Preprocessing complete.\")\n",
    "    return tokenized_corpus\n",
    "\n",
    "def build_vocabulary(corpus):\n",
    "    \"\"\"Builds vocabulary and word-to-index mappings.\"\"\"\n",
    "    print(\"Building vocabulary...\")\n",
    "    word_counts = Counter(word for doc in corpus for word in doc)\n",
    "\n",
    "    sorted_and_filtered = [item for item in word_counts.most_common() if item[1] >= MIN_WORD_COUNT]\n",
    "    \n",
    "    vocabulary = [word for word, count in sorted_and_filtered[:VOCAB_SIZE]]\n",
    "    \n",
    "    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n",
    "    idx_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "    print(\"Vocabulary built.\")\n",
    "    return vocabulary, word_to_idx, idx_to_word\n",
    "\n",
    "print(\"Preprocessing functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3673199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preparation ---\n",
      "Preprocessing documents...\n",
      "Preprocessing complete.\n",
      "Building vocabulary...\n",
      "Vocabulary built.\n",
      "\n",
      "Data preparation complete. Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Data Preparation ---\")\n",
    "docs = load_data(DATASET_PATH)\n",
    "tokenized_corpus = preprocess(docs)\n",
    "vocabulary, word_to_idx, idx_to_word = build_vocabulary(tokenized_corpus)\n",
    "print(f\"\\nData preparation complete. Vocabulary size: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99123287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VSM & SVD helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def build_cooccurrence_matrix(corpus, word_to_idx, window_size=5):\n",
    "    \"\"\"Builds a word-word co-occurrence matrix.\"\"\"\n",
    "    vocab_size = len(word_to_idx)\n",
    "    cooc_matrix = lil_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    print(\"Building co-occurrence matrix (this may take a moment)...\")\n",
    "    for doc in corpus:\n",
    "        doc_indices = [word_to_idx[word] for word in doc if word in word_to_idx]\n",
    "        for i, target_idx in enumerate(doc_indices):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc_indices), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    context_idx = doc_indices[j]\n",
    "                    cooc_matrix[target_idx, context_idx] += 1\n",
    "    \n",
    "    print(\"Co-occurrence matrix built.\")\n",
    "    return cooc_matrix.tocsr() \n",
    "\n",
    "def calculate_ppmi(cooc_matrix):\n",
    "    \"\"\"Calculates the Positive Pointwise Mutual Information (PPMI) matrix.\"\"\"\n",
    "    total_cooccurrences = cooc_matrix.sum()\n",
    "    word_counts = np.array(cooc_matrix.sum(axis=1)).flatten()\n",
    "    word_counts[word_counts == 0] = 1\n",
    "    p_w_c = cooc_matrix / total_cooccurrences\n",
    "    p_w = word_counts / total_cooccurrences\n",
    "    pmi = np.log2(p_w_c.toarray() / (p_w[:, None] * p_w[None, :]) + 1e-9) \n",
    "    ppmi_matrix = np.maximum(0, pmi)\n",
    "    \n",
    "    return csr_matrix(ppmi_matrix)\n",
    "\n",
    "def get_most_similar(query_word, word_vectors, word_to_idx, idx_to_word, top_n=5):\n",
    "    if query_word not in word_to_idx:\n",
    "        return [f\"'{query_word}' not in vocabulary\"] * top_n\n",
    "        \n",
    "    query_idx = word_to_idx[query_word]\n",
    "    query_vector = word_vectors[query_idx].reshape(1, -1)\n",
    "    \n",
    "    similarities = cosine_similarity(query_vector, word_vectors).flatten()\n",
    "    top_indices = np.argsort(-similarities)[1:top_n+1]\n",
    "    \n",
    "    return [idx_to_word[i] for i in top_indices]\n",
    "\n",
    "print(\" VSM & SVD helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad13691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Task A: VSM & SVD Execution ---\n",
      "Building co-occurrence matrix...\n",
      "Building co-occurrence matrix (this may take a moment)...\n",
      "Co-occurrence matrix built.\n",
      "\n",
      "Calculating PPMI matrix...\n",
      "Final PPMI matrix dimensions: (10000, 10000)\n",
      "\n",
      "Applying Truncated SVD...\n",
      "SVD-reduced matrix dimensions: (10000, 300)\n",
      "\n",
      " Task A models are ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Task A: VSM & SVD Execution ---\")\n",
    "print(\"Building co-occurrence matrix...\")\n",
    "cooc_matrix = build_cooccurrence_matrix(tokenized_corpus, word_to_idx, window_size=WINDOW_SIZE)\n",
    "\n",
    "print(\"\\nCalculating PPMI matrix...\")\n",
    "ppmi_matrix = calculate_ppmi(cooc_matrix)\n",
    "print(f\"Final PPMI matrix dimensions: {ppmi_matrix.shape}\")\n",
    "\n",
    "print(\"\\nApplying Truncated SVD...\")\n",
    "svd = TruncatedSVD(n_components=SVD_DIMS, random_state=RANDOM_SEED)\n",
    "svd_vectors = svd.fit_transform(ppmi_matrix)\n",
    "print(f\"SVD-reduced matrix dimensions: {svd_vectors.shape}\")\n",
    "\n",
    "print(\"\\n Task A models are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741eb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VSM vs. SVD Similarity Results ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query Word</th>\n",
       "      <th>Top 5 (VSM - PPMI)</th>\n",
       "      <th>Top 5 (SVD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>market</td>\n",
       "      <td>stock, housing, growth, prices, sales</td>\n",
       "      <td>stock, share, analysts, growth, prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film</td>\n",
       "      <td>best, awards, actress, director, actor</td>\n",
       "      <td>films, movie, best, actress, director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>election</td>\n",
       "      <td>general, labour, campaign, blair, party</td>\n",
       "      <td>labour, general, partys, labours, campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>game</td>\n",
       "      <td>games, play, players, match, playing</td>\n",
       "      <td>play, games, playing, players, first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>software</td>\n",
       "      <td>microsoft, programs, users, antivirus, windows</td>\n",
       "      <td>programs, microsoft, windows, users, program</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Query Word                              Top 5 (VSM - PPMI)  \\\n",
       "0     market           stock, housing, growth, prices, sales   \n",
       "1       film          best, awards, actress, director, actor   \n",
       "2   election         general, labour, campaign, blair, party   \n",
       "3       game            games, play, players, match, playing   \n",
       "4   software  microsoft, programs, users, antivirus, windows   \n",
       "\n",
       "                                    Top 5 (SVD)  \n",
       "0        stock, share, analysts, growth, prices  \n",
       "1         films, movie, best, actress, director  \n",
       "2    labour, general, partys, labours, campaign  \n",
       "3          play, games, playing, players, first  \n",
       "4  programs, microsoft, windows, users, program  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_words = ['market', 'film', 'election', 'game', 'software']\n",
    "results_data = []\n",
    "\n",
    "for word in query_words:\n",
    "    vsm_sim = get_most_similar(word, ppmi_matrix, word_to_idx, idx_to_word)\n",
    "    svd_sim = get_most_similar(word, svd_vectors, word_to_idx, idx_to_word)\n",
    "    results_data.append({\n",
    "        \"Query Word\": word,\n",
    "        \"Top 5 (VSM - PPMI)\": ', '.join(vsm_sim),\n",
    "        \"Top 5 (SVD)\": ', '.join(svd_sim)\n",
    "    })\n",
    "results_df_A = pd.DataFrame(results_data)\n",
    "print(\"--- VSM vs. SVD Similarity Results ---\")\n",
    "display(results_df_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44141d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Model and Dataset classes defined.\n"
     ]
    }
   ],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for generating skip-gram pairs and negative samples.\"\"\"\n",
    "    def __init__(self, corpus, word_to_idx, word_counts, window_size=5, num_neg_samples=5):\n",
    "        self.data = []\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        \n",
    "        freq = np.array([word_counts[word] for word in vocabulary])**0.75\n",
    "        self.sampling_dist = freq / freq.sum()\n",
    "        \n",
    "        print(\"Creating Skip-gram dataset (this might take a minute)...\")\n",
    "\n",
    "        for doc in corpus:\n",
    "            doc_indices = [word_to_idx[word] for word in doc if word in word_to_idx]\n",
    "            for i, target_word_idx in enumerate(doc_indices):\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(doc_indices), i + window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context_word_idx = doc_indices[j]\n",
    "                        self.data.append((target_word_idx, context_word_idx))\n",
    "        print(\"Dataset creation complete.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.data[idx]\n",
    "        # Draw negative samples from the frequency distribution\n",
    "        neg_samples = np.random.choice(\n",
    "            len(self.sampling_dist),\n",
    "            size=self.num_neg_samples,\n",
    "            p=self.sampling_dist\n",
    "        )\n",
    "        return torch.tensor(target), torch.tensor(context), torch.from_numpy(neg_samples)\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    \"\"\"PyTorch implementation of Skip-gram with Negative Sampling.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.init_embeds()\n",
    "\n",
    "    def init_embeds(self):\n",
    "        # Initialize embeddings with a uniform distribution\n",
    "        initrange = 0.5 / self.target_embeddings.embedding_dim\n",
    "        self.target_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.context_embeddings.weight.data.uniform_(-0, 0) # Context vectors initialized to zero\n",
    "\n",
    "    def forward(self, target, context, neg_samples):\n",
    "        target_embed = self.target_embeddings(target)\n",
    "        context_embed = self.context_embeddings(context)\n",
    "        neg_embed = self.context_embeddings(neg_samples)\n",
    "\n",
    "        # Positive score (dot product between target and true context)\n",
    "        pos_score = torch.sum(target_embed * context_embed, dim=1)\n",
    "        pos_loss = -torch.nn.functional.logsigmoid(pos_score).mean()\n",
    "\n",
    "        # Negative score (dot products between target and negative samples)\n",
    "        neg_score = torch.bmm(neg_embed, target_embed.unsqueeze(2)).squeeze()\n",
    "        neg_loss = -torch.nn.functional.logsigmoid(-neg_score).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "print(\"Word2Vec Model and Dataset classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32385611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Task B: Word2Vec Training ---\n",
      "Creating Skip-gram dataset (this might take a minute)...\n",
      "Dataset creation complete.\n",
      "Starting model training...\n",
      "Epoch 1, Loss: 1.2512\n",
      "Epoch 2, Loss: 1.0957\n",
      "Epoch 3, Loss: 1.0230\n",
      "Epoch 4, Loss: 0.9925\n",
      "Epoch 5, Loss: 0.9758\n",
      "Epoch 6, Loss: 0.9657\n",
      "Epoch 7, Loss: 0.9597\n",
      "Epoch 8, Loss: 0.9552\n",
      "Epoch 9, Loss: 0.9515\n",
      "Epoch 10, Loss: 0.9496\n",
      "Epoch 11, Loss: 0.9480\n",
      "Epoch 12, Loss: 0.9458\n",
      "Epoch 13, Loss: 0.9457\n",
      "Epoch 14, Loss: 0.9447\n",
      "Epoch 15, Loss: 0.9437\n",
      "Epoch 16, Loss: 0.9436\n",
      "Epoch 17, Loss: 0.9432\n",
      "Epoch 18, Loss: 0.9431\n",
      "Epoch 19, Loss: 0.9428\n",
      "Epoch 20, Loss: 0.9432\n",
      "Epoch 21, Loss: 0.9422\n",
      "Epoch 22, Loss: 0.9423\n",
      "Epoch 23, Loss: 0.9428\n",
      "Epoch 24, Loss: 0.9419\n",
      "Epoch 25, Loss: 0.9427\n",
      "Epoch 26, Loss: 0.9426\n",
      "Epoch 27, Loss: 0.9425\n",
      "Epoch 28, Loss: 0.9427\n",
      "Epoch 29, Loss: 0.9430\n",
      "Epoch 30, Loss: 0.9425\n",
      "Epoch 31, Loss: 0.9431\n",
      "Epoch 32, Loss: 0.9426\n",
      "Epoch 33, Loss: 0.9427\n",
      "Epoch 34, Loss: 0.9430\n",
      "Epoch 35, Loss: 0.9434\n",
      "Epoch 36, Loss: 0.9433\n",
      "Epoch 37, Loss: 0.9432\n",
      "Epoch 38, Loss: 0.9433\n",
      "Epoch 39, Loss: 0.9434\n",
      "Epoch 40, Loss: 0.9434\n",
      "Epoch 41, Loss: 0.9450\n",
      "Epoch 42, Loss: 0.9442\n",
      "Epoch 43, Loss: 0.9443\n",
      "Epoch 44, Loss: 0.9446\n",
      "Epoch 45, Loss: 0.9446\n",
      "Epoch 46, Loss: 0.9451\n",
      "Epoch 47, Loss: 0.9449\n",
      "Epoch 48, Loss: 0.9452\n",
      "Epoch 49, Loss: 0.9454\n",
      "Epoch 50, Loss: 0.9450\n",
      "\n",
      " Word2Vec model training complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Task B: Word2Vec Training ---\")\n",
    "\n",
    "# Get word counts needed for negative sampling distribution\n",
    "full_word_counts = Counter(word for doc in tokenized_corpus for word in doc)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Word2VecDataset(\n",
    "    tokenized_corpus, word_to_idx, full_word_counts, window_size=WINDOW_SIZE\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SkipGramNegativeSampling(vocab_size=len(vocabulary), embed_dim=W2V_DIMS)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "model.train()\n",
    "print(\"Starting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # --- MODIFIED LINE: Removed tqdm wrapper from dataloader ---\n",
    "    for target, context, neg_samples in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target, context, neg_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "w2v_vectors = model.target_embeddings.weight.data.cpu().numpy()\n",
    "print(\"\\n Word2Vec model training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50522f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Word Similarity Comparison ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n",
      "<>:34: SyntaxWarning: 'tuple' object is not callable; perhaps you missed a comma?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ---1.Final Similarity Comparison\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Final Word Similarity Comparison ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m w2v_sim_results = [\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(get_most_similar(q, w2v_vectors, word_to_idx, idx_to_word)) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[43mquery_words\u001b[49m]\n\u001b[32m     24\u001b[39m results_df_A[\u001b[33m'\u001b[39m\u001b[33mTop 5 (Word2Vec)\u001b[39m\u001b[33m'\u001b[39m] = w2v_sim_results\n\u001b[32m     25\u001b[39m display(results_df_A)\n",
      "\u001b[31mNameError\u001b[39m: name 'query_words' is not defined"
     ]
    }
   ],
   "source": [
    "def solve_analogy(w1, w2, w3, word_vectors, word_to_idx, idx_to_word):\n",
    "    \"\"\"Solves word analogy: w1 is to w2 as w3 is to ?\"\"\"\n",
    "    if not all(w in word_to_idx for w in [w1, w2, w3]):\n",
    "        return \"One or more words not in vocabulary.\"\n",
    "\n",
    "    vec1 = word_vectors[word_to_idx[w1]]\n",
    "    vec2 = word_vectors[word_to_idx[w2]]\n",
    "    vec3 = word_vectors[word_to_idx[w3]]\n",
    "\n",
    "    analogy_vec = vec2 - vec1 + vec3\n",
    "    similarities = cosine_similarity(analogy_vec.reshape(1, -1), word_vectors).flatten()\n",
    "    \n",
    "    # Exclude query words from results\n",
    "    for w in [w1, w2, w3]:\n",
    "        similarities[word_to_idx[w]] = -np.inf\n",
    "        \n",
    "    # Find the single best answer\n",
    "    answer_idx = np.argmax(similarities)\n",
    "    return idx_to_word[answer_idx]\n",
    "\n",
    "# ---1.Final Similarity Comparison\n",
    "print(\"--- Final Word Similarity Comparison ---\")\n",
    "w2v_sim_results = [', '.join(get_most_similar(q, w2v_vectors, word_to_idx, idx_to_word)) for q in query_words]\n",
    "results_df_A['Top 5 (Word2Vec)'] = w2v_sim_results\n",
    "display(results_df_A)\n",
    "\n",
    "# --- 2. Word Analogy Tasks ---\n",
    "print(\"\\n--- Word Analogy Tasks (Word2Vec) ---\")\n",
    "analogies = [\n",
    "    (\"business\", \"profit\", \"politics\"),     \n",
    "    (\"britain\", \"london\", \"france\"),\n",
    "    (\"sport\", \"football\", \"tech\"),\n",
    "    (\"minister\", \"government\", \"player\"),\n",
    "    (\"movie\", \"entertainment\", \"computer\")\n",
    "    # (\"movie\", \"entertainment\", \"computer\")    \n",
    "]\n",
    "for w1, w2, w3 in analogies:\n",
    "    answer = solve_analogy(w1, w2, w3, w2v_vectors, word_to_idx, idx_to_word)\n",
    "    print(f\"'{w1}' is to '{w2}' as '{w3}' is to ---> '{answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1f7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
