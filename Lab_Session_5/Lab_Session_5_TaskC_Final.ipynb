{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Session 5 - Task C: Document Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ensure NLTK data is available\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: bbc\n",
            "\n",
            "Loaded 2225 documents.\n",
            "Class distribution: Counter({'sport': 511, 'business': 510, 'politics': 417, 'tech': 401, 'entertainment': 386})\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and tokenizes text, returning a single string.\"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) \n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([word for word in tokens if word not in stop_words and len(word) > 2])\n",
        "\n",
        "def load_labeled_data(folder_path):\n",
        "    \"\"\"Loads documents and their corresponding category labels.\"\"\"\n",
        "    all_docs = []\n",
        "    all_labels = []\n",
        "    print(f\"Loading data from: {folder_path}\")\n",
        "    for category in os.listdir(folder_path):\n",
        "        category_path = os.path.join(folder_path, category)\n",
        "        if not os.path.isdir(category_path): continue\n",
        "        for filename in os.listdir(category_path):\n",
        "            file_path = os.path.join(category_path, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                all_docs.append(preprocess_text(f.read()))\n",
        "                all_labels.append(category)\n",
        "    return all_docs, all_labels\n",
        "\n",
        "BBC_FOLDER = 'bbc'\n",
        "documents, labels = load_labeled_data(BBC_FOLDER)\n",
        "print(f\"\\nLoaded {len(documents)} documents.\")\n",
        "print(f\"Class distribution: {Counter(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Feature Extraction (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the TF-IDF feature matrix: (2225, 5000)\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(documents) \n",
        "y = np.array(labels) \n",
        "\n",
        "print(f\"Shape of the TF-IDF feature matrix: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Data Splitting and Cross-Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 1780 documents\n",
            "Test set size: 445 documents\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} documents\")\n",
        "print(f\"Test set size: {X_test.shape[0]} documents\")\n",
        "\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Task C.a: Train & Evaluate Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Logistic Regression ---\n",
            "--- Cross-Validation Results per Fold ---\n",
            "Fold 1: Accuracy=0.9803, F1-Score=0.9799\n",
            "Fold 2: Accuracy=0.9663, F1-Score=0.9653\n",
            "Fold 3: Accuracy=0.9635, F1-Score=0.9636\n",
            "Fold 4: Accuracy=0.9747, F1-Score=0.9748\n",
            "Fold 5: Accuracy=0.9860, F1-Score=0.9861\n",
            "\n",
            "--- Average Cross-Validation Scores ---\n",
            "Avg Accuracy: 0.9742\n",
            "Avg Precision: 0.9754\n",
            "Avg Recall: 0.9729\n",
            "Avg F1-Score: 0.9739\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, X, y, cv):\n",
        "    \"\"\"A helper function to perform 5-fold cross-validation and print results.\"\"\"\n",
        "    scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "    \n",
        "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring_metrics)\n",
        "    \n",
        "    print(\"--- Cross-Validation Results per Fold ---\")\n",
        "    for i in range(cv.get_n_splits()):\n",
        "        print(f\"Fold {i+1}: Accuracy={scores['test_accuracy'][i]:.4f}, F1-Score={scores['test_f1_macro'][i]:.4f}\")\n",
        "    \n",
        "    print(\"\\n--- Average Cross-Validation Scores ---\")\n",
        "    avg_scores = {\n",
        "        'Avg Accuracy': np.mean(scores['test_accuracy']),\n",
        "        'Avg Precision': np.mean(scores['test_precision_macro']),\n",
        "        'Avg Recall': np.mean(scores['test_recall_macro']),\n",
        "        'Avg F1-Score': np.mean(scores['test_f1_macro'])\n",
        "    }\n",
        "    for metric, value in avg_scores.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "    \n",
        "    return avg_scores\n",
        "\n",
        "print(\"--- Evaluating Logistic Regression ---\")\n",
        "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_avg_scores = evaluate_model(lr_classifier, X_train, y_train, cv_strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Task C.b: Train & Evaluate Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Support Vector Machine (SVM) ---\n",
            "--- Cross-Validation Results per Fold ---\n",
            "Fold 1: Accuracy=0.9775, F1-Score=0.9767\n",
            "Fold 2: Accuracy=0.9719, F1-Score=0.9708\n",
            "Fold 3: Accuracy=0.9691, F1-Score=0.9688\n",
            "Fold 4: Accuracy=0.9747, F1-Score=0.9745\n",
            "Fold 5: Accuracy=0.9860, F1-Score=0.9861\n",
            "\n",
            "--- Average Cross-Validation Scores ---\n",
            "Avg Accuracy: 0.9758\n",
            "Avg Precision: 0.9766\n",
            "Avg Recall: 0.9747\n",
            "Avg F1-Score: 0.9754\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Evaluating Support Vector Machine (SVM) ---\")\n",
        "svm_classifier = SVC(random_state=42)\n",
        "svm_avg_scores = evaluate_model(svm_classifier, X_train, y_train, cv_strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Compare Models and Discuss\n",
        "\n",
        "We now directly compare the average scores from both models to fulfill the final requirement of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Side-by-Side Model Comparison (Average CV Scores) ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Avg Accuracy</th>\n",
              "      <th>Avg Precision</th>\n",
              "      <th>Avg Recall</th>\n",
              "      <th>Avg F1-Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <td>0.974157</td>\n",
              "      <td>0.975409</td>\n",
              "      <td>0.972905</td>\n",
              "      <td>0.973940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.975843</td>\n",
              "      <td>0.976554</td>\n",
              "      <td>0.974666</td>\n",
              "      <td>0.975386</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Avg Accuracy  Avg Precision  Avg Recall  Avg F1-Score\n",
              "Logistic Regression      0.974157       0.975409    0.972905      0.973940\n",
              "SVM                      0.975843       0.976554    0.974666      0.975386"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Discussion ---\n",
            "Based on the average F1-score, the SVM performed slightly better. \n",
            "Both models achieved excellent performance (>97% accuracy), suggesting that TF-IDF features are highly effective for this topic classification task.\n",
            "The slight edge for SVM is common in high-dimensional text classification, as SVMs are powerful at finding optimal separating hyperplanes in such feature spaces.\n"
          ]
        }
      ],
      "source": [
        "comparison_df = pd.DataFrame([lr_avg_scores, svm_avg_scores],\n",
        "                             index=['Logistic Regression', 'SVM'])\n",
        "\n",
        "print(\"--- Side-by-Side Model Comparison (Average CV Scores) ---\")\n",
        "display(comparison_df)\n",
        "\n",
        "print(\"\\n--- Discussion ---\")\n",
        "best_f1_model = comparison_df['Avg F1-Score'].idxmax()\n",
        "print(f\"Based on the average F1-score, the {best_f1_model} performed slightly better. \")\n",
        "print(\"Both models achieved excellent performance (>97% accuracy), suggesting that TF-IDF features are highly effective for this topic classification task.\")\n",
        "print(\"The slight edge for SVM is common in high-dimensional text classification, as SVMs are powerful at finding optimal separating hyperplanes in such feature spaces.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Save Final Models for Future Use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vectorizer and both final models have been saved successfully to disk.\n"
          ]
        }
      ],
      "source": [
        "lr_classifier.fit(X_train, y_train)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "with open('logistic_regression_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lr_classifier, f)\n",
        "\n",
        "with open('svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svm_classifier, f)\n",
        "\n",
        "print(\"Vectorizer and both final models have been saved successfully to disk.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cs561",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
