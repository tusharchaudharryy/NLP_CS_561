{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6f2199",
   "metadata": {},
   "source": [
    "Task 3: Implement Byte-Pair Encoding (BPE) and WordPiece\n",
    "tokenizers from scratch, using the provided documents as your\n",
    "training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba25ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "583882f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=30000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = []\n",
    "        self.merges = {}\n",
    "\n",
    "    def get_word_freqs(self, corpus):\n",
    "        full_text = \" \".join(corpus)\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', full_text.lower())\n",
    "        return Counter(words)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        word_freqs = self.get_word_freqs(corpus)\n",
    "        alphabet = set()\n",
    "\n",
    "        for word in word_freqs:\n",
    "            alphabet.update(list(word))\n",
    "        self.vocab = sorted(alphabet)\n",
    "\n",
    "        splits = {word: list(word) + ['</w>'] for word in word_freqs}\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = defaultdict(int)\n",
    "            for word, freq in word_freqs.items():\n",
    "                word_splits = splits[word]\n",
    "                for i in range(len(word_splits) - 1):\n",
    "                    pair = (word_splits[i], word_splits[i+1])\n",
    "                    pair_freqs[pair] += freq\n",
    "\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "            merged_token = \"\".join(best_pair)\n",
    "            self.vocab.append(merged_token)\n",
    "            self.merges[best_pair] = merged_token\n",
    "\n",
    "            for word in word_freqs:\n",
    "                word_splits = splits[word]\n",
    "                new_split = []\n",
    "                i = 0\n",
    "                while i < len(word_splits):\n",
    "                    if i < len(word_splits) - 1 and (word_splits[i], word_splits[i+1]) == best_pair:\n",
    "                        new_split.append(merged_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_split.append(word_splits[i])\n",
    "                        i += 1\n",
    "                splits[word] = new_split\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "        tokenized_output = []\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                tokenized_output.append(word)\n",
    "                continue\n",
    "\n",
    "            word_splits = list(word) + ['</w>']\n",
    "            for pair, merge in self.merges.items():\n",
    "                new_split = []\n",
    "                i = 0\n",
    "                while i < len(word_splits):\n",
    "                    if i < len(word_splits) - 1 and (word_splits[i], word_splits[i+1]) == pair:\n",
    "                        new_split.append(merge)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_split.append(word_splits[i])\n",
    "                        i += 1\n",
    "                word_splits = new_split\n",
    "\n",
    "            tokenized_output.extend(word_splits)\n",
    "\n",
    "        return tokenized_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfbf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=30000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = []\n",
    "        self.unknown_token = \"[UNK]\"\n",
    "\n",
    "    def get_word_freqs(self, corpus):\n",
    "        full_text = \" \".join(corpus)\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', full_text.lower())\n",
    "        return Counter(words)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        word_freqs = self.get_word_freqs(corpus)\n",
    "\n",
    "        alphabet = set()\n",
    "        for word in word_freqs:\n",
    "            alphabet.update(list(word))\n",
    "        self.vocab = sorted(alphabet) + [self.unknown_token]\n",
    "\n",
    "        splits = {word: list(word) for word in word_freqs}\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            scores = defaultdict(float)\n",
    "            pair_freqs = defaultdict(int)\n",
    "            char_freqs = defaultdict(int)\n",
    "\n",
    "            for word, freq in word_freqs.items():\n",
    "                word_splits = splits[word]\n",
    "                for i in range(len(word_splits) - 1):\n",
    "                    pair = (word_splits[i], word_splits[i+1])\n",
    "                    pair_freqs[pair] += freq\n",
    "                    char_freqs[word_splits[i]] += freq\n",
    "                if word_splits:\n",
    "                    char_freqs[word_splits[-1]] += freq\n",
    "\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if char_freqs[pair[0]] > 0 and char_freqs[pair[1]] > 0:\n",
    "                    scores[pair] = freq / (char_freqs[pair[0]] * char_freqs[pair[1]])\n",
    "\n",
    "            if not scores:\n",
    "                break\n",
    "\n",
    "            best_pair = max(scores, key=scores.get)\n",
    "            first, second = best_pair\n",
    "            merged_token = first + second.replace('##', '')\n",
    "            merged_token_with_prefix = '##' + merged_token if not first.startswith('##') else merged_token\n",
    "\n",
    "            self.vocab.append(merged_token)\n",
    "            self.vocab.append(merged_token_with_prefix)\n",
    "\n",
    "            for word in word_freqs:\n",
    "                word_splits = splits[word]\n",
    "                new_split = []\n",
    "                i = 0\n",
    "                while i < len(word_splits):\n",
    "                    if i < len(word_splits) - 1 and (word_splits[i], word_splits[i+1]) == best_pair:\n",
    "                        current_merged = word_splits[i] + word_splits[i+1].replace('##', '')\n",
    "                        new_split.append(current_merged)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_split.append(word_splits[i])\n",
    "                        i += 1\n",
    "                splits[word] = new_split\n",
    "\n",
    "        self.vocab = sorted(set(self.vocab))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower()\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "        tokenized_output = []\n",
    "        for word in words:\n",
    "            if word in self.vocab:\n",
    "                tokenized_output.append(word)\n",
    "                continue\n",
    "\n",
    "            subwords = []\n",
    "            current_pos = 0\n",
    "            while current_pos < len(word):\n",
    "                found_subword = None\n",
    "                prefix = '##' if current_pos > 0 else ''\n",
    "\n",
    "                for end_pos in range(len(word), current_pos, -1):\n",
    "                    sub = prefix + word[current_pos:end_pos]\n",
    "                    if sub in self.vocab:\n",
    "                        found_subword = sub\n",
    "                        break\n",
    "\n",
    "                if found_subword:\n",
    "                    subwords.append(found_subword)\n",
    "                    current_pos += len(found_subword) - len(prefix)\n",
    "                else:\n",
    "                    subwords.append(self.unknown_token)\n",
    "                    current_pos += 1\n",
    "\n",
    "            tokenized_output.extend(subwords)\n",
    "\n",
    "        return tokenized_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad512e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Byte-Pair Encoding (BPE) Tokenizer\n",
      "============================================================\n",
      "\n",
      "BPE Tokenization Examples:\n",
      "Sentence: 'Learning models need lots of data.'\n",
      "Tokens: ['lea', 'r', 'ning</w>', 'mo', 'de', 'l', 's</w>', 'ne', 'ed</w>', 'lo', 'ts</w>', 'of</w>', 'da', 't', 'a</w>', '.']\n",
      "\n",
      "Sentence: 'Tokenization helps language models handle new words.'\n",
      "Tokens: ['to', 'ken', 'i', 'z', 'ation</w>', 'h', 'el', 'p', 's</w>', 'l', 'an', 'gu', 'ag', 'e</w>', 'mo', 'de', 'l', 's</w>', 'han', 'd', 'le</w>', 'new</w>', 'wor', 'ds</w>', '.']\n",
      "\n",
      "Sample BPE Vocabulary: ['0m</w>', 'race</w>', 'ted</w>', 'pro', 'rop', 'won</w>', 'ru', 'lea', 'de', 'ken', 'med', 'ship', '7</w>', 'ke</w>', 'ere</w>', 'tion', 'se</w>', 'it', 'champion', 'championship']\n",
      "\n",
      "============================================================\n",
      "WordPiece Tokenizer\n",
      "============================================================\n",
      "\n",
      "WordPiece Tokenization Examples:\n",
      "Sentence: 'Learning models need lots of data.'\n",
      "Tokens: ['l', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'm', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'n', '[UNK]', '[UNK]', '[UNK]', 'l', '[UNK]', '[UNK]', '[UNK]', 'o', '[UNK]', 'd', '[UNK]', '[UNK]', '[UNK]', '.']\n",
      "\n",
      "Sentence: 'Tokenization helps language models handle new words.'\n",
      "Tokens: ['t', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'h', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'l', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'm', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'h', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'n', '[UNK]', '[UNK]', 'w', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.']\n",
      "\n",
      "Sample WordPiece Vocabulary: ['##qat', '##qata', '##qatar', '##qu', '##rix', '##rouj', '##rrouj', '##sampl', '##sample', '##samples', '##subj', '##subje', '##subjec', '##subject', '##temp', '##ubj', '##uerrouj', '##uj', '##xamp', '##xampl']\n"
     ]
    }
   ],
   "source": [
    "K = 500\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    documents = []\n",
    "    try:\n",
    "        file_list = sorted(os.listdir(directory_path))\n",
    "    except (IOError, AttributeError, FileNotFoundError):\n",
    "        return []\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "bbc_folder_path = 'BBC'\n",
    "corpus = load_documents_from_directory(bbc_folder_path)\n",
    "\n",
    "if not corpus:\n",
    "    print(f\"Could not find or read files in directory '{bbc_folder_path}'.\")\n",
    "else:\n",
    "    test_sentences = [\n",
    "        \"Learning models need lots of data.\",\n",
    "        \"Tokenization helps language models handle new words.\"\n",
    "    ]\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Byte-Pair Encoding (BPE) Tokenizer\")\n",
    "    print(\"=\"*60)\n",
    "    bpe_tokenizer = BPETokenizer(vocab_size=K)\n",
    "    bpe_tokenizer.train(corpus)\n",
    "\n",
    "    print(\"\\nBPE Tokenization Examples:\")\n",
    "    for sentence in test_sentences:\n",
    "        tokens = bpe_tokenizer.tokenize(sentence)\n",
    "        print(f\"Sentence: '{sentence}'\")\n",
    "        print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "    print(\"Sample BPE Vocabulary:\", bpe_tokenizer.vocab[200:220])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WordPiece Tokenizer\")\n",
    "    print(\"=\"*60)\n",
    "    wp_tokenizer = WordPieceTokenizer(vocab_size=K)\n",
    "    wp_tokenizer.train(corpus)\n",
    "\n",
    "    print(\"\\nWordPiece Tokenization Examples:\")\n",
    "    for sentence in test_sentences:\n",
    "        tokens = wp_tokenizer.tokenize(sentence)\n",
    "        print(f\"Sentence: '{sentence}'\")\n",
    "        print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "    print(\"Sample WordPiece Vocabulary:\", wp_tokenizer.vocab[200:220])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8740ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
