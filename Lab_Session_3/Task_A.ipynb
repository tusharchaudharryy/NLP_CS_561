{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b33b330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tushar\\anaconda3\\envs\\cs561\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Using cached numpy-2.2.6-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, scikit-learn\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.3.2\n",
      "\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "    Uninstalling numpy-2.3.2:\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   ------------- -------------------------- 1/3 [numpy]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed numpy-2.2.6 scikit-learn-1.7.1 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Tushar\\anaconda3\\envs\\cs561\\Lib\\site-packages\\~~mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Tushar\\anaconda3\\envs\\cs561\\Lib\\site-packages\\~~mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f9663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463330a",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8dbefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(folder_path='BBC', unk_threshold=1):\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    all_docs = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                all_docs.append(f.read())\n",
    "\n",
    "    full_text = \"\\n\".join(all_docs)\n",
    "    tokenized_sentences = [word_tokenize(s.lower()) for s in sent_tokenize(full_text)]\n",
    "    \n",
    "    word_counts = Counter(token for sent in tokenized_sentences for token in sent)\n",
    "    vocab = {word for word, count in word_counts.items() if count > unk_threshold}\n",
    "    vocab.add('<UNK>')\n",
    "    \n",
    "    processed_sentences = [\n",
    "        [word if word in vocab else '<UNK>' for word in sent] \n",
    "        for sent in tokenized_sentences\n",
    "    ]\n",
    "    \n",
    "    print(f\"Data loaded. Sentences: {len(processed_sentences)}, Vocab size: {len(vocab)}\")\n",
    "    return processed_sentences, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3aca71",
   "metadata": {},
   "source": [
    "n-gram Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04fa0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, n, vocab):\n",
    "        self.n = n\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.counts = defaultdict(Counter)\n",
    "        self.context_counts = Counter()\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sent in sentences:\n",
    "            padded_sent = ['<s>'] * (self.n - 1) + sent + ['</s>']\n",
    "            for i in range(len(padded_sent) - self.n + 1):\n",
    "                ngram = tuple(padded_sent[i : i + self.n])\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                self.counts[context][word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "    def get_prob(self, word, context):\n",
    "        context_tuple = tuple(context)\n",
    "        word_count = self.counts[context_tuple].get(word, 0)\n",
    "        context_count = self.context_counts.get(context_tuple, 0)\n",
    "        return (word_count + 1) / (context_count + self.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cffb4b",
   "metadata": {},
   "source": [
    "Interpolated Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0085081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedModel:\n",
    "    def __init__(self, models, vocab):\n",
    "        self.models = models\n",
    "        self.n = max(model.n for model in models)\n",
    "        self.lambdas = np.ones(len(models)) / len(models)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def train(self, held_out_sentences, iterations=10):\n",
    "        print(\"Learning interpolation weights with EM algorithm...\")\n",
    "        for i in range(iterations):\n",
    "            expected_counts = np.zeros(len(self.models))\n",
    "            for sent in held_out_sentences:\n",
    "                padded_sent = ['<s>'] * (self.n - 1) + sent + ['</s>']\n",
    "                for j in range(self.n - 1, len(padded_sent)):\n",
    "                    probs = np.zeros(len(self.models))\n",
    "                    for k, model in enumerate(self.models):\n",
    "                        context = padded_sent[j - model.n + 1 : j]\n",
    "                        word = padded_sent[j]\n",
    "                        probs[k] = model.get_prob(word, context)\n",
    "                    \n",
    "                    total_prob = np.dot(self.lambdas, probs)\n",
    "                    if total_prob > 1e-9:\n",
    "                        for k in range(len(self.models)):\n",
    "                            expected_counts[k] += (self.lambdas[k] * probs[k]) / total_prob\n",
    "            \n",
    "            self.lambdas = expected_counts / np.sum(expected_counts)\n",
    "        print(f\"EM complete. Final lambdas: {self.lambdas}\")\n",
    "\n",
    "    def get_prob(self, word, context):\n",
    "        interpolated_prob = 0.0\n",
    "        for i, model in enumerate(self.models):\n",
    "            model_context = context[-(model.n - 1):] if model.n > 1 else []\n",
    "            interpolated_prob += self.lambdas[i] * model.get_prob(word, model_context)\n",
    "        return interpolated_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0dc34",
   "metadata": {},
   "source": [
    "Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98458f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, test_sentences):\n",
    "    total_log_prob = 0\n",
    "    token_count = 0\n",
    "    \n",
    "    for sent in test_sentences:\n",
    "        padded_sent = ['<s>'] * (model.n - 1) + sent + ['</s>']\n",
    "        token_count += len(sent) + 1 \n",
    "        for i in range(model.n - 1, len(padded_sent)):\n",
    "            context = padded_sent[i - model.n + 1 : i]\n",
    "            word = padded_sent[i]\n",
    "            prob = model.get_prob(word, context)\n",
    "            if prob > 0:\n",
    "                total_log_prob += np.log2(prob)\n",
    "\n",
    "    perplexity = 2 ** (-total_log_prob / token_count)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89170fd7",
   "metadata": {},
   "source": [
    "Model Evalutaion and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eefb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data loaded. Sentences: 239, Vocab size: 585\n",
      "\n",
      "Data split complete:\n",
      "Training sentences: 171\n",
      "Validation sentences (for EM): 20\n",
      "Testing sentences: 48\n",
      "\n",
      "Training n-gram Models \n",
      "Unigram model trained.\n",
      "Bigram model trained.\n",
      "Trigram model trained.\n",
      "\n",
      "Training Interpolated Model \n",
      "Learning interpolation weights with EM algorithm...\n",
      "EM complete. Final lambdas: [0.52372313 0.45702263 0.01925424]\n",
      "\n",
      " Calculating Perplexity on the Test Set \n",
      "\n",
      " Perplexity Score Evaluation Results \n",
      "                 Model  Perplexity Score\n",
      "         Unigram (n=1)        175.281094\n",
      "          Bigram (n=2)        213.990381\n",
      "         Trigram (n=3)        403.637043\n",
      "Interpolated (n=1,2,3)        151.038585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_sentences, vocab = load_and_preprocess_data(folder_path='BBC')\n",
    "random.shuffle(all_sentences)\n",
    "split_idx = int(0.8 * len(all_sentences))\n",
    "train_full, test_set = all_sentences[:split_idx], all_sentences[split_idx:]\n",
    "\n",
    "val_split_idx = int(0.9 * len(train_full))\n",
    "train_set, val_set = train_full[:val_split_idx], train_full[val_split_idx:]\n",
    "\n",
    "print(f\"\\nData split complete:\")\n",
    "print(f\"Training sentences: {len(train_set)}\")\n",
    "print(f\"Validation sentences (for EM): {len(val_set)}\")\n",
    "print(f\"Testing sentences: {len(test_set)}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining n-gram Models \")\n",
    "unigram_model = NgramLanguageModel(n=1, vocab=vocab)\n",
    "unigram_model.train(train_set)\n",
    "print(\"Unigram model trained.\")\n",
    "\n",
    "bigram_model = NgramLanguageModel(n=2, vocab=vocab)\n",
    "bigram_model.train(train_set)\n",
    "print(\"Bigram model trained.\")\n",
    "\n",
    "trigram_model = NgramLanguageModel(n=3, vocab=vocab)\n",
    "trigram_model.train(train_set)\n",
    "print(\"Trigram model trained.\")\n",
    "\n",
    "print(\"\\nTraining Interpolated Model \")\n",
    "interpolated_model = InterpolatedModel(\n",
    "    models=[unigram_model, bigram_model, trigram_model], \n",
    "    vocab=vocab\n",
    ")\n",
    "interpolated_model.train(val_set)\n",
    "\n",
    "print(\"\\n Calculating Perplexity on the Test Set \")\n",
    "perplexity_unigram = calculate_perplexity(unigram_model, test_set)\n",
    "perplexity_bigram = calculate_perplexity(bigram_model, test_set)\n",
    "perplexity_trigram = calculate_perplexity(trigram_model, test_set)\n",
    "perplexity_interpolated = calculate_perplexity(interpolated_model, test_set)\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Unigram (n=1)\", \"Bigram (n=2)\", \"Trigram (n=3)\", \"Interpolated (n=1,2,3)\"],\n",
    "    \"Perplexity Score\": [perplexity_unigram, perplexity_bigram, perplexity_trigram, perplexity_interpolated]\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n Perplexity Score Evaluation Results \")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f1172",
   "metadata": {},
   "source": [
    "## Evaluation and Analysis\n",
    "\n",
    "| Model                | Perplexity Score |\n",
    "|-----------------------|------------------|\n",
    "| Unigram (n=1)         | 175.281094 |\n",
    "| Bigram (n=2)          | 213.990381 |\n",
    "| Trigram (n=3)         | 403.637043 |\n",
    "| Interpolated (n=1,2,3)| 151.038585 |\n",
    "\n",
    "- **Unigram** works reasonably well since it avoids sparsity, but ignores context.  \n",
    "- **Bigram** is worse due to sparsity in a small dataset.  \n",
    "- **Trigram** performs the worst, showing the effect of extreme sparsity.  \n",
    "- **Interpolated model** outperforms all individual models, confirming interpolation alleviates sparsity and balances coverage + context.  \n",
    "- Learned weights (λ values) typically favor unigram and bigram more heavily, showing trigram contributes very little in small corpora.  \n",
    "\n",
    "**Conclusion:** Interpolation significantly improves language model performance by combining multiple n-gram models and mitigating sparsity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a565c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
