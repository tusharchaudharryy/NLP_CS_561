{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1427ae29",
   "metadata": {},
   "source": [
    "Task 2: Stemming and Lemmatization of the documents using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615e045c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\tushar\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tushar\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tushar\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tushar\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading regex-2025.7.34-cp310-cp310-win_amd64.whl (276 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.1 regex-2025.7.34\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe10c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3277c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ac705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_directory(directory_path):\n",
    "    documents = []\n",
    "    filenames = []\n",
    "    try:\n",
    "        file_list = sorted(os.listdir(directory_path), key=lambda x: int(re.search(r'(\\d+)', x).group(1)))\n",
    "    except (IOError, AttributeError, FileNotFoundError):\n",
    "        print(f\"Error: Could not find or read files in directory '{directory_path}'.\")\n",
    "        return [], []\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                documents.append(f.read())\n",
    "                filenames.append(filename)\n",
    "    return documents, filenames\n",
    "\n",
    "def base_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b', 'EMAIL', text)\n",
    "    text = re.sub(r'\\b\\d{4}[-/]\\d{2}[-/]\\d{2}\\b', 'DATE', text)\n",
    "    text = re.sub(r'\\b\\d+[\\d.,-]*\\b', 'NUM', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "def compute_tf(preprocessed_corpus):\n",
    "    tf_scores = []\n",
    "    for doc_tokens in preprocessed_corpus:\n",
    "        doc_len = len(doc_tokens)\n",
    "        if doc_len == 0:\n",
    "            tf_scores.append({})\n",
    "            continue\n",
    "        word_counts = Counter(doc_tokens)\n",
    "        tf_doc = {word: count / doc_len for word, count in word_counts.items()}\n",
    "        tf_scores.append(tf_doc)\n",
    "    return tf_scores\n",
    "\n",
    "def compute_idf(preprocessed_corpus, vocabulary):\n",
    "    if not preprocessed_corpus or not vocabulary:\n",
    "        return {}\n",
    "    N = len(preprocessed_corpus)\n",
    "    df = {word: 0 for word in vocabulary}\n",
    "    for word in vocabulary:\n",
    "        for doc_tokens in preprocessed_corpus:\n",
    "            if word in doc_tokens:\n",
    "                df[word] += 1\n",
    "    idf_scores = {word: np.log(N / count) for word, count in df.items() if count > 0}\n",
    "    return idf_scores\n",
    "\n",
    "def compute_tfidf(tf_scores, idf_scores):\n",
    "    tfidf_scores = []\n",
    "    for doc_tf in tf_scores:\n",
    "        tfidf_doc = {word: tf * idf_scores.get(word, 0) for word, tf in doc_tf.items()}\n",
    "        tfidf_scores.append(tfidf_doc)\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "100e7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_with_stemming(text):\n",
    "    tokens = base_preprocess(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def preprocess_with_lemmatization(text):\n",
    "    tokens = base_preprocess(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55874b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "\n",
      "Vocabulary Size Comparison:\n",
      "Original:     1247\n",
      "Lemmatized:   1172\n",
      "Stemmed:      1078\n",
      "\n",
      "TF-IDF (Lemmatized Corpus):\n",
      "          claxton   hunting     first     major     medal   british   hurdler  \\\n",
      "001.txt  0.099858  0.014265  0.022992  0.018068  0.019997  0.009998  0.010965   \n",
      "002.txt  0.000000  0.000000  0.011177  0.000000  0.000000  0.000000  0.000000   \n",
      "003.txt  0.000000  0.000000  0.000000  0.000000  0.002792  0.000000  0.000000   \n",
      "004.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "005.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "006.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "007.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "008.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.004486  0.000000   \n",
      "009.txt  0.000000  0.000000  0.004191  0.000000  0.002734  0.002734  0.005996   \n",
      "010.txt  0.000000  0.000000  0.000000  0.000000  0.004199  0.004199  0.000000   \n",
      "\n",
      "            sarah        is  confident  \n",
      "001.txt  0.014265  0.001548   0.015328  \n",
      "002.txt  0.000000  0.001129   0.000000  \n",
      "003.txt  0.000000  0.000432   0.000000  \n",
      "004.txt  0.000000  0.000846   0.000000  \n",
      "005.txt  0.000000  0.000000   0.000000  \n",
      "006.txt  0.000000  0.001113   0.000000  \n",
      "007.txt  0.000000  0.000000   0.000000  \n",
      "008.txt  0.000000  0.002778   0.006878  \n",
      "009.txt  0.000000  0.000423   0.000000  \n",
      "010.txt  0.000000  0.000650   0.000000  \n",
      "\n",
      "==================================================\n",
      "\n",
      "TF-IDF (Stemmed Corpus):\n",
      "          claxton      hunt     first     major     medal   british   hurdler  \\\n",
      "001.txt  0.099858  0.014265  0.022992  0.018068  0.019997  0.009998  0.010965   \n",
      "002.txt  0.000000  0.000000  0.011177  0.000000  0.000000  0.000000  0.000000   \n",
      "003.txt  0.000000  0.000000  0.000000  0.000000  0.002792  0.000000  0.000000   \n",
      "004.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "005.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "006.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "007.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "008.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.004486  0.000000   \n",
      "009.txt  0.000000  0.000000  0.004191  0.000000  0.002734  0.002734  0.005996   \n",
      "010.txt  0.000000  0.000000  0.000000  0.000000  0.004199  0.004199  0.000000   \n",
      "\n",
      "            sarah        is    confid  \n",
      "001.txt  0.014265  0.001548  0.011466  \n",
      "002.txt  0.000000  0.001129  0.000000  \n",
      "003.txt  0.000000  0.000432  0.000000  \n",
      "004.txt  0.000000  0.000846  0.000000  \n",
      "005.txt  0.000000  0.000000  0.000000  \n",
      "006.txt  0.000000  0.001113  0.000000  \n",
      "007.txt  0.000000  0.000000  0.000000  \n",
      "008.txt  0.000000  0.002778  0.005145  \n",
      "009.txt  0.000000  0.000423  0.000000  \n",
      "010.txt  0.000000  0.000650  0.000000  \n"
     ]
    }
   ],
   "source": [
    "bbc_folder_path = 'BBC'\n",
    "corpus, doc_names = load_documents_from_directory(bbc_folder_path)\n",
    "\n",
    "if corpus:\n",
    "    original_processed_corpus = [base_preprocess(doc) for doc in corpus]\n",
    "    stemmed_corpus = [preprocess_with_stemming(doc) for doc in corpus]\n",
    "    lemmatized_corpus = [preprocess_with_lemmatization(doc) for doc in corpus]\n",
    "\n",
    "    print(\"Preprocessing complete.\\n\")\n",
    "\n",
    "    vocab_original = sorted(set(token for doc in original_processed_corpus for token in doc))\n",
    "    vocab_lemmatized = sorted(set(token for doc in lemmatized_corpus for token in doc))\n",
    "    vocab_stemmed = sorted(set(token for doc in stemmed_corpus for token in doc))\n",
    "\n",
    "    print(\"Vocabulary Size Comparison:\")\n",
    "    print(f\"Original:     {len(vocab_original)}\")\n",
    "    print(f\"Lemmatized:   {len(vocab_lemmatized)}\")\n",
    "    print(f\"Stemmed:      {len(vocab_stemmed)}\\n\")\n",
    "\n",
    "    print(\"TF-IDF (Lemmatized Corpus):\")\n",
    "    tf_lemmatized = compute_tf(lemmatized_corpus)\n",
    "    idf_lemmatized = compute_idf(lemmatized_corpus, vocab_lemmatized)\n",
    "    tfidf_lemmatized = compute_tfidf(tf_lemmatized, idf_lemmatized)\n",
    "    df_tfidf_lemmatized = pd.DataFrame(tfidf_lemmatized).fillna(0)\n",
    "    df_tfidf_lemmatized.index = doc_names\n",
    "    print(df_tfidf_lemmatized.iloc[:10, :10])\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"TF-IDF (Stemmed Corpus):\")\n",
    "    tf_stemmed = compute_tf(stemmed_corpus)\n",
    "    idf_stemmed = compute_idf(stemmed_corpus, vocab_stemmed)\n",
    "    tfidf_stemmed = compute_tfidf(tf_stemmed, idf_stemmed)\n",
    "    df_tfidf_stemmed = pd.DataFrame(tfidf_stemmed).fillna(0)\n",
    "    df_tfidf_stemmed.index = doc_names\n",
    "    print(df_tfidf_stemmed.iloc[:10, :10])\n",
    "\n",
    "else:\n",
    "    print(\"No documents loaded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
