{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5805321",
   "metadata": {},
   "source": [
    "Task 1: Loading Documents and Computing TF, IDF, TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e14f2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71574197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_directory(directory_path):\n",
    "    documents = []\n",
    "    filenames = []\n",
    "    try:\n",
    "        file_list = sorted(os.listdir(directory_path), key=lambda x: int(re.search(r'(\\d+)', x).group(1)))\n",
    "    except (IOError, AttributeError):\n",
    "        print(f\"Error: Could not find or read files in directory '{directory_path}'.\")\n",
    "        return [], []\n",
    "\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                documents.append(f.read())\n",
    "                filenames.append(filename)\n",
    "    return documents, filenames\n",
    "\n",
    "bbc_folder_path = 'BBC'\n",
    "corpus, doc_names = load_documents_from_directory(bbc_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f066d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing Complete ---\n",
      "Loaded and preprocessed 20 documents.\n",
      "Example tokens from first document (001.txt): ['claxton', 'hunting', 'first', 'major', 'medal', 'british', 'hurdler', 'sarah', 'claxton', 'is', 'confident', 'she', 'can', 'win', 'her', 'first', 'major', 'medal', 'at', 'next']\n",
      "\n",
      "--- Vocabulary Size ---\n",
      "Total number of unique words (vocabulary size): 1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b', 'EMAIL', text)\n",
    "    text = re.sub(r'\\b\\d{4}[-/]\\d{2}[-/]\\d{2}\\b', 'DATE', text)\n",
    "    text = re.sub(r'\\b\\d+[\\d.,-]*\\b', 'NUM', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "if corpus:\n",
    "    preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
    "    print(\"--- Preprocessing Complete ---\")\n",
    "    print(f\"Loaded and preprocessed {len(preprocessed_corpus)} documents.\")\n",
    "    print(f\"Example tokens from first document ({doc_names[0]}): {preprocessed_corpus[0][:20]}\\n\")\n",
    "else:\n",
    "    preprocessed_corpus = []\n",
    "\n",
    "if preprocessed_corpus:\n",
    "    all_tokens = [token for doc_tokens in preprocessed_corpus for token in doc_tokens]\n",
    "    vocabulary = sorted(list(set(all_tokens)))\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    print(f\"--- Vocabulary Size ---\")\n",
    "    print(f\"Total number of unique words (vocabulary size): {vocabulary_size}\\n\")\n",
    "else:\n",
    "    vocabulary = []\n",
    "    vocabulary_size = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e248c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(preprocessed_corpus):\n",
    "    tf_scores = []\n",
    "    for doc_tokens in preprocessed_corpus:\n",
    "        doc_len = len(doc_tokens)\n",
    "        if doc_len == 0:\n",
    "            tf_scores.append({})\n",
    "            continue\n",
    "        word_counts = Counter(doc_tokens)\n",
    "        tf_doc = {word: count / doc_len for word, count in word_counts.items()}\n",
    "        tf_scores.append(tf_doc)\n",
    "    return tf_scores\n",
    "\n",
    "def compute_idf(preprocessed_corpus, vocabulary):\n",
    "    if not preprocessed_corpus or not vocabulary:\n",
    "        return {}\n",
    "    N = len(preprocessed_corpus)\n",
    "    df = {word: 0 for word in vocabulary}\n",
    "    for word in vocabulary:\n",
    "        for doc_tokens in preprocessed_corpus:\n",
    "            if word in doc_tokens:\n",
    "                df[word] += 1\n",
    "    idf_scores = {word: np.log(N / count) for word, count in df.items() if count > 0}\n",
    "    return idf_scores\n",
    "\n",
    "def compute_tfidf(tf_scores, idf_scores):\n",
    "    tfidf_scores = []\n",
    "    for doc_tf in tf_scores:\n",
    "        tfidf_doc = {word: tf * idf_scores.get(word, 0) for word, tf in doc_tf.items()}\n",
    "        tfidf_scores.append(tfidf_doc)\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae235f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Custom TF-IDF Matrix (Top 10 words) ---\n",
      "          claxton   hunting     first     major     medal   british   hurdler  \\\n",
      "001.txt  0.099858  0.014265  0.022992  0.018068  0.019997  0.009998  0.010965   \n",
      "002.txt  0.000000  0.000000  0.011177  0.000000  0.000000  0.000000  0.000000   \n",
      "003.txt  0.000000  0.000000  0.000000  0.000000  0.002792  0.000000  0.000000   \n",
      "004.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "005.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "006.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "007.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "008.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.004486  0.000000   \n",
      "009.txt  0.000000  0.000000  0.004191  0.000000  0.002734  0.002734  0.005996   \n",
      "010.txt  0.000000  0.000000  0.000000  0.000000  0.004199  0.004199  0.000000   \n",
      "\n",
      "            sarah        is  confident  \n",
      "001.txt  0.014265  0.001548   0.015328  \n",
      "002.txt  0.000000  0.001129   0.000000  \n",
      "003.txt  0.000000  0.000432   0.000000  \n",
      "004.txt  0.000000  0.000846   0.000000  \n",
      "005.txt  0.000000  0.000000   0.000000  \n",
      "006.txt  0.000000  0.001113   0.000000  \n",
      "007.txt  0.000000  0.000000   0.000000  \n",
      "008.txt  0.000000  0.002778   0.006878  \n",
      "009.txt  0.000000  0.000423   0.000000  \n",
      "010.txt  0.000000  0.000650   0.000000  \n",
      "\n",
      "\n",
      "--- Scikit-learn TF-IDF Matrix (Top 10 words) ---\n",
      "          claxton   hunting     first     major     medal   british   hurdler  \\\n",
      "001.txt  0.507822  0.072546  0.158134  0.115083  0.170150  0.085075  0.063769   \n",
      "002.txt  0.000000  0.000000  0.068183  0.000000  0.000000  0.000000  0.000000   \n",
      "003.txt  0.000000  0.000000  0.000000  0.000000  0.028677  0.000000  0.000000   \n",
      "004.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "005.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "006.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "007.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "008.txt  0.000000  0.000000  0.000000  0.000000  0.000000  0.040230  0.000000   \n",
      "009.txt  0.000000  0.000000  0.036834  0.000000  0.029725  0.029725  0.044561   \n",
      "010.txt  0.000000  0.000000  0.000000  0.000000  0.039460  0.039460  0.000000   \n",
      "\n",
      "            sarah        is  confident  \n",
      "001.txt  0.072546  0.049967   0.105423  \n",
      "002.txt  0.000000  0.032317   0.000000  \n",
      "003.txt  0.000000  0.016843   0.000000  \n",
      "004.txt  0.000000  0.025744   0.000000  \n",
      "005.txt  0.000000  0.000000   0.000000  \n",
      "006.txt  0.000000  0.037063   0.000000  \n",
      "007.txt  0.000000  0.000000   0.000000  \n",
      "008.txt  0.000000  0.094514   0.049853  \n",
      "009.txt  0.000000  0.017458   0.000000  \n",
      "010.txt  0.000000  0.023176   0.000000  \n",
      "\n",
      "\n",
      "--- IDF Score Comparison (Custom vs. Sklearn) ---\n",
      "Note: Sklearn uses a smoothed formula: log((N+1)/(df+1)) + 1\n",
      "        Custom IDF  Sklearn IDF\n",
      "1000m     2.995732     3.351375\n",
      "100m      1.386294     2.252763\n",
      "10k       2.995732     3.351375\n",
      "10km      2.995732     3.351375\n",
      "10secs    2.995732     3.351375\n",
      "1200m     2.995732     3.351375\n",
      "12th      2.995732     3.351375\n",
      "200m      1.609438     2.435085\n",
      "37cm      2.995732     3.351375\n",
      "3km       2.995732     3.351375\n",
      "\n",
      "\n",
      "--- Top 10 TF-IDF Words for Document: 001.txt ---\n",
      "\n",
      "Custom Implementation:\n",
      "claxton      0.099858\n",
      "hurdles      0.032894\n",
      "first        0.022992\n",
      "medal        0.019997\n",
      "her          0.019012\n",
      "major        0.018068\n",
      "confident    0.015328\n",
      "struggled    0.014265\n",
      "translate    0.014265\n",
      "trailing     0.014265\n",
      "Name: 001.txt, dtype: float64\n",
      "\n",
      "Scikit-learn Implementation:\n",
      "claxton     0.507822\n",
      "the         0.303053\n",
      "hurdles     0.191307\n",
      "her         0.188535\n",
      "medal       0.170150\n",
      "first       0.158134\n",
      "has         0.131104\n",
      "major       0.115083\n",
      "european    0.113121\n",
      "as          0.113121\n",
      "Name: 001.txt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if preprocessed_corpus:\n",
    "    tf_scores_custom = compute_tf(preprocessed_corpus)\n",
    "    idf_scores_custom = compute_idf(preprocessed_corpus, vocabulary)\n",
    "    tfidf_scores_custom = compute_tfidf(tf_scores_custom, idf_scores_custom)\n",
    "\n",
    "    df_tfidf_custom = pd.DataFrame(tfidf_scores_custom).fillna(0)\n",
    "    df_tfidf_custom.index = doc_names\n",
    "    print(\"--- Custom TF-IDF Matrix (Top 10 words) ---\")\n",
    "    print(df_tfidf_custom.iloc[:10, :10])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    processed_docs_str = [' '.join(tokens) for tokens in preprocessed_corpus]\n",
    "    tfidf_vec = TfidfVectorizer(smooth_idf=True, norm='l2')\n",
    "    tfidf_sklearn_matrix = tfidf_vec.fit_transform(processed_docs_str)\n",
    "    \n",
    "    df_tfidf_sklearn = pd.DataFrame(tfidf_sklearn_matrix.toarray(), columns=tfidf_vec.get_feature_names_out(), index=doc_names)\n",
    "    print(\"--- Scikit-learn TF-IDF Matrix (Top 10 words) ---\")\n",
    "    sklearn_vocab = tfidf_vec.get_feature_names_out()\n",
    "    common_vocab_slice = [word for word in df_tfidf_custom.columns[:10] if word in sklearn_vocab]\n",
    "    if common_vocab_slice:\n",
    "        print(df_tfidf_sklearn.iloc[:10][common_vocab_slice])\n",
    "    else:\n",
    "        print(\"No common vocabulary in the slice to display.\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    df_idf_sklearn = pd.DataFrame(tfidf_vec.idf_, index=tfidf_vec.get_feature_names_out(), columns=['Sklearn IDF'])\n",
    "    df_idf_custom = pd.DataFrame.from_dict(idf_scores_custom, orient='index', columns=['Custom IDF'])\n",
    "    df_idf_compare = df_idf_custom.join(df_idf_sklearn).reindex(df_idf_sklearn.index).head(10)\n",
    "    print(\"--- IDF Score Comparison (Custom vs. Sklearn) ---\")\n",
    "    print(\"Note: Sklearn uses a smoothed formula: log((N+1)/(df+1)) + 1\")\n",
    "    print(df_idf_compare)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    doc_to_check = doc_names[0]\n",
    "    top_words_custom = df_tfidf_custom.loc[doc_to_check].sort_values(ascending=False).head(10)\n",
    "    top_words_sklearn = df_tfidf_sklearn.loc[doc_to_check].sort_values(ascending=False).head(10)\n",
    "    print(f\"--- Top 10 TF-IDF Words for Document: {doc_to_check} ---\")\n",
    "    print(\"\\nCustom Implementation:\")\n",
    "    print(top_words_custom)\n",
    "    print(\"\\nScikit-learn Implementation:\")\n",
    "    print(top_words_sklearn)\n",
    "else:\n",
    "    print(\"Execution halted because no documents were loaded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
