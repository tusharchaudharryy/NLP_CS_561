{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7fe5cd",
   "metadata": {},
   "source": [
    "# Lab Session 6 - Task 1: Spam Classification with RNN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec1f8e",
   "metadata": {},
   "source": [
    "1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5813c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n",
      "Reconstructing vocabulary from BBC dataset...\n",
      "Vocabulary of size 10002 created.\n",
      "'word2vec_embeddings.npy' loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar\\AppData\\Local\\Temp\\ipykernel_12588\\3701710793.py:104: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('spam.csv', sep='\\\\t', header=None, names=['message', 'label'], on_bad_lines='skip', encoding='latin-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and processed. Total samples: 1547\n",
      "Selected dataset split ratio: Train: 1237 (80.0%), Test: 310 (20.0%)\n",
      "\n",
      "----- Evaluating RNN (LSTM) using device: cpu -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for RNN (LSTM):  20%|██        | 1/5 [00:11<00:46, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for RNN (LSTM):  40%|████      | 2/5 [00:20<00:29,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for RNN (LSTM):  60%|██████    | 3/5 [00:28<00:18,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for RNN (LSTM):  80%|████████  | 4/5 [00:37<00:08,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for RNN (LSTM): 100%|██████████| 5/5 [00:45<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Metrics: F1-Score=0.0000\n",
      "\n",
      "----- Evaluating CNN using device: cpu -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for CNN:  20%|██        | 1/5 [00:02<00:08,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for CNN:  40%|████      | 2/5 [00:04<00:06,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for CNN:  60%|██████    | 3/5 [00:06<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test Metrics: F1-Score=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for CNN:  80%|████████  | 4/5 [00:08<00:02,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test Metrics: F1-Score=0.6536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CV Folds for CNN: 100%|██████████| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test Metrics: F1-Score=0.6522\n",
      "\n",
      "--- Final Model Performance Comparison (Mean ± Std Dev over 5 Folds) ---\n",
      "                 accuracy  precision    recall  f1_score\n",
      "RNN (LSTM) mean  0.515484   0.000000  0.000000  0.000000\n",
      "           std   0.001443   0.000000  0.000000  0.000000\n",
      "CNN        mean  0.503226   0.193862  0.400000  0.261154\n",
      "           std   0.016290   0.265456  0.547723  0.357600\n",
      "\n",
      "**Best model by F1-score:** CNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lab Session 6 - Task 1: Spam Classification (Final Version)\n",
    "\n",
    "This script builds, trains, and compares an RNN (LSTM) and a CNN for SMS spam classification \n",
    "using pre-trained word embeddings.\n",
    "\n",
    "Workflow:\n",
    "1. Setup: Import libraries and apply environment fixes.\n",
    "2. Load Embeddings: Reconstruct the vocabulary and load pre-trained embeddings.\n",
    "3. Load & Preprocess Spam Data: Clean and convert text to padded sequences.\n",
    "4. Split Data: Train/test split and PyTorch DataLoaders.\n",
    "5. Define Models: RNN (LSTM) and CNN architectures.\n",
    "6. Run Cross-Validation: 5-fold training and evaluation.\n",
    "7. Analyze Results: Present comparison and discussion.\n",
    "\"\"\"\n",
    "\n",
    "# ================================\n",
    "# 1. Imports and Setup\n",
    "# ================================\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Fix for the OMP: Error #15 on some systems\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Setup complete.\")\n",
    "\n",
    "# ================================\n",
    "# 2. Load Pre-trained Embeddings and Vocabulary\n",
    "# ================================\n",
    "def bbc_preprocess_for_vocab(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "BBC_FOLDER = 'bbc'\n",
    "VOCAB_SIZE = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print(\"Reconstructing vocabulary from BBC dataset...\")\n",
    "all_tokens = []\n",
    "for category in os.listdir(BBC_FOLDER):\n",
    "    category_path = os.path.join(BBC_FOLDER, category)\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "    for filename in os.listdir(category_path):\n",
    "        with open(os.path.join(category_path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            all_tokens.extend(bbc_preprocess_for_vocab(f.read()))\n",
    "\n",
    "word_counts = Counter(all_tokens)\n",
    "vocab = [word for word, _ in word_counts.most_common(VOCAB_SIZE)]\n",
    "\n",
    "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<PAD>'] = 0\n",
    "word_to_idx['<UNK>'] = 1\n",
    "print(f\"Vocabulary of size {len(word_to_idx)} created.\")\n",
    "\n",
    "try:\n",
    "    numpy_weights = np.load('word2vec_embeddings.npy')\n",
    "    pretrained_weights = torch.from_numpy(numpy_weights)\n",
    "    print(\"'word2vec_embeddings.npy' loaded successfully.\")\n",
    "    final_pretrained_weights = torch.cat([torch.randn(2, EMBEDDING_DIM), pretrained_weights])\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'word2vec_embeddings.npy' not found. Using random embeddings.\")\n",
    "    final_pretrained_weights = torch.randn(len(word_to_idx), EMBEDDING_DIM)\n",
    "\n",
    "# ================================\n",
    "# 3. Load and Preprocess SMS Spam Dataset\n",
    "# ================================\n",
    "def sms_preprocess(text):\n",
    "    \"\"\"Basic cleaning and tokenization for SMS messages.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Load dataset\n",
    "# --- FIX IS HERE: The column names have been swapped to match the file format ---\n",
    "df = pd.read_csv('spam.csv', sep='\\\\t', header=None, names=['message', 'label'], on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "# Clean up any rows that were not parsed correctly or have missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# The labels are already 0s and 1s, so we just need to ensure they are integers.\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Convert text to sequences of integer IDs. Use 1 for <UNK> if word is not in vocab\n",
    "sequences = [[word_to_idx.get(word, 1) for word in sms_preprocess(msg)] for msg in df['message']]\n",
    "\n",
    "# Pad sequences to a fixed length for batching\n",
    "MAX_SEQ_LENGTH = 50\n",
    "padded_sequences = np.zeros((len(sequences), MAX_SEQ_LENGTH), dtype=np.int64)\n",
    "for i, seq in enumerate(sequences):\n",
    "    seq_len = len(seq)\n",
    "    if seq_len > 0:\n",
    "        if seq_len < MAX_SEQ_LENGTH:\n",
    "            padded_sequences[i, -seq_len:] = np.array(seq)\n",
    "        else:\n",
    "            padded_sequences[i, :] = np.array(seq[:MAX_SEQ_LENGTH])\n",
    "\n",
    "X = padded_sequences\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Dataset loaded and processed. Total samples: {len(X)}\")\n",
    "\n",
    "# ================================\n",
    "# 4. Split Data and Create DataLoaders\n",
    "# ================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Selected dataset split ratio: Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%), \"\n",
    "      f\"Test: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.LongTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "test_dataset = SpamDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# ================================\n",
    "# 5. Model Architectures (RNN and CNN)\n",
    "# ================================\n",
    "def create_embedding_layer(weights, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer\n",
    "\n",
    "class SpamRNN(nn.Module):\n",
    "    def __init__(self, pretrained_weights, hidden_dim=128, n_layers=2):\n",
    "        super(SpamRNN, self).__init__()\n",
    "        self.embedding = create_embedding_layer(pretrained_weights)\n",
    "        self.lstm = nn.LSTM(EMBEDDING_DIM, hidden_dim, n_layers, batch_first=True, \n",
    "                            dropout=0.5, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpamCNN(nn.Module):\n",
    "    def __init__(self, pretrained_weights, n_filters=100, filter_sizes=[2,3,4]):\n",
    "        super(SpamCNN, self).__init__()\n",
    "        self.embedding = create_embedding_layer(pretrained_weights)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=EMBEDDING_DIM, out_channels=n_filters, kernel_size=fs) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        conved = [torch.relu(conv(x)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        out = self.fc(cat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# ================================\n",
    "# 6. Training and Evaluation with 5-Fold Cross-Validation\n",
    "# ================================\n",
    "def run_cross_validation(model_class, model_name, pretrained_weights, X_train, y_train, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n----- Evaluating {model_name} using device: {device} -----\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train, y_train), total=5, desc=f\"CV Folds for {model_name}\")):\n",
    "        model = model_class(pretrained_weights).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]\n",
    "        train_dataset_fold = SpamDataset(X_train_fold, y_train_fold)\n",
    "        train_loader_fold = DataLoader(train_dataset_fold, batch_size=64, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(5):\n",
    "            for inputs, labels in train_loader_fold:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                preds = (outputs > 0.5).int().cpu().numpy()\n",
    "                all_labels.extend(labels.cpu().numpy().astype(int))\n",
    "                all_preds.extend(preds)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(all_labels, all_preds),\n",
    "            'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "            'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "            'f1_score': f1_score(all_labels, all_preds, zero_division=0)\n",
    "        }\n",
    "        fold_results.append(metrics)\n",
    "        print(f\"Fold {fold+1} Test Metrics: F1-Score={metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(fold_results)\n",
    "\n",
    "# Run CV for both models\n",
    "rnn_results_df = run_cross_validation(SpamRNN, \"RNN (LSTM)\", final_pretrained_weights, X_train, y_train, test_loader)\n",
    "cnn_results_df = run_cross_validation(SpamCNN, \"CNN\", final_pretrained_weights, X_train, y_train, test_loader)\n",
    "\n",
    "# ================================\n",
    "# 7. Final Results and Discussion\n",
    "# ================================\n",
    "rnn_summary = rnn_results_df.agg(['mean', 'std'])\n",
    "cnn_summary = cnn_results_df.agg(['mean', 'std'])\n",
    "\n",
    "comparison_df = pd.concat([rnn_summary, cnn_summary], keys=['RNN (LSTM)', 'CNN'])\n",
    "\n",
    "print(\"\\n--- Final Model Performance Comparison (Mean ± Std Dev over 5 Folds) ---\")\n",
    "print(comparison_df)\n",
    "\n",
    "best_f1_model = comparison_df.xs('mean', level=1)['f1_score'].idxmax()\n",
    "print(f\"\\n**Best model by F1-score:** {best_f1_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55eb883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
