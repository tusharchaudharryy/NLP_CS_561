{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "928ec639",
   "metadata": {},
   "source": [
    "# Task C: Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd21f6",
   "metadata": {},
   "source": [
    "1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee30d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497afd",
   "metadata": {},
   "source": [
    "2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0e7bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: bbc\n",
      "\n",
      "Loaded 2225 documents.\n",
      "Example document: 'sales boost time warner profit quarterly profits media giant timewarner jumped three months december...'\n",
      "Label for example document: business\n",
      "Class distribution: Counter({'sport': 511, 'business': 510, 'politics': 417, 'tech': 401, 'entertainment': 386})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([word for word in tokens if word not in stop_words and len(word) > 2])\n",
    "\n",
    "def load_labeled_data(folder_path):\n",
    "    \"\"\"Loads documents and their corresponding labels.\"\"\"\n",
    "    all_docs = []\n",
    "    all_labels = []\n",
    "    print(f\"Loading data from: {folder_path}\")\n",
    "    for category in os.listdir(folder_path):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        for filename in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                all_docs.append(preprocess_text(f.read()))\n",
    "                all_labels.append(category)\n",
    "    return all_docs, all_labels\n",
    "\n",
    "BBC_FOLDER = \"bbc\"  \n",
    "documents, labels = load_labeled_data(BBC_FOLDER)\n",
    "\n",
    "print(f\"\\nLoaded {len(documents)} documents.\")\n",
    "print(f\"Example document: '{documents[0][:100]}...'\")\n",
    "print(f\"Label for example document: {labels[0]}\")\n",
    "print(f\"Class distribution: {Counter(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8394c12",
   "metadata": {},
   "source": [
    "3. Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1d144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF feature matrix: (2225, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(documents)  \n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Shape of the TF-IDF feature matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280db6ae",
   "metadata": {},
   "source": [
    "4. Data Splitting & CV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc53d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1780 documents\n",
      "Test set size: 445 documents\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} documents\")\n",
    "print(f\"Test set size: {X_test.shape[0]} documents\")\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec7478",
   "metadata": {},
   "source": [
    "5. Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d63fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, cv):\n",
    "    \"\"\"Performs 5-fold cross-validation and prints results.\"\"\"\n",
    "    scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    scores = cross_validate(model, X, y, cv=cv, scoring=scoring_metrics)\n",
    "\n",
    "    print(\" Cross-Validation Results per Fold \")\n",
    "    for i in range(cv.get_n_splits()):\n",
    "        print(f\"Fold {i+1}:\")\n",
    "        print(f\"  Accuracy: {scores['test_accuracy'][i]:.4f}\")\n",
    "        print(f\"  Precision: {scores['test_precision_macro'][i]:.4f}\")\n",
    "        print(f\"  Recall: {scores['test_recall_macro'][i]:.4f}\")\n",
    "        print(f\"  F1-Score: {scores['test_f1_macro'][i]:.4f}\")\n",
    "\n",
    "    avg_scores = {\n",
    "        'Avg Accuracy': np.mean(scores['test_accuracy']),\n",
    "        'Avg Precision': np.mean(scores['test_precision_macro']),\n",
    "        'Avg Recall': np.mean(scores['test_recall_macro']),\n",
    "        'Avg F1-Score': np.mean(scores['test_f1_macro']),\n",
    "    }\n",
    "\n",
    "    print(\"\\n Average Cross-Validation Scores \")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881319ab",
   "metadata": {},
   "source": [
    "6. Task C.a: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027e038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Logistic Regression \n",
      " Cross-Validation Results per Fold \n",
      "Fold 1:\n",
      "  Accuracy: 0.9803\n",
      "  Precision: 0.9803\n",
      "  Recall: 0.9796\n",
      "  F1-Score: 0.9799\n",
      "Fold 2:\n",
      "  Accuracy: 0.9663\n",
      "  Precision: 0.9663\n",
      "  Recall: 0.9644\n",
      "  F1-Score: 0.9653\n",
      "Fold 3:\n",
      "  Accuracy: 0.9635\n",
      "  Precision: 0.9644\n",
      "  Recall: 0.9630\n",
      "  F1-Score: 0.9636\n",
      "Fold 4:\n",
      "  Accuracy: 0.9747\n",
      "  Precision: 0.9777\n",
      "  Recall: 0.9731\n",
      "  F1-Score: 0.9748\n",
      "Fold 5:\n",
      "  Accuracy: 0.9860\n",
      "  Precision: 0.9884\n",
      "  Recall: 0.9844\n",
      "  F1-Score: 0.9861\n",
      "\n",
      " Average Cross-Validation Scores \n",
      "Avg Accuracy: 0.9742\n",
      "Avg Precision: 0.9754\n",
      "Avg Recall: 0.9729\n",
      "Avg F1-Score: 0.9739\n",
      "\n",
      " Final Performance on Hold-Out Test Set \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      0.98      0.99       102\n",
      "entertainment       0.99      1.00      0.99        77\n",
      "     politics       0.99      0.98      0.98        84\n",
      "        sport       0.99      1.00      1.00       102\n",
      "         tech       0.99      1.00      0.99        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating Logistic Regression \")\n",
    "lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_avg_scores = evaluate_model(lr_classifier, X_train, y_train, cv_strategy)\n",
    "\n",
    "print(\"\\n Final Performance on Hold-Out Test Set \")\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb033f4a",
   "metadata": {},
   "source": [
    "7. Task C.b: SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070e6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Support Vector Machine (SVM) \n",
      " Cross-Validation Results per Fold \n",
      "Fold 1:\n",
      "  Accuracy: 0.9775\n",
      "  Precision: 0.9771\n",
      "  Recall: 0.9765\n",
      "  F1-Score: 0.9767\n",
      "Fold 2:\n",
      "  Accuracy: 0.9719\n",
      "  Precision: 0.9721\n",
      "  Recall: 0.9701\n",
      "  F1-Score: 0.9708\n",
      "Fold 3:\n",
      "  Accuracy: 0.9691\n",
      "  Precision: 0.9685\n",
      "  Recall: 0.9695\n",
      "  F1-Score: 0.9688\n",
      "Fold 4:\n",
      "  Accuracy: 0.9747\n",
      "  Precision: 0.9767\n",
      "  Recall: 0.9729\n",
      "  F1-Score: 0.9745\n",
      "Fold 5:\n",
      "  Accuracy: 0.9860\n",
      "  Precision: 0.9884\n",
      "  Recall: 0.9844\n",
      "  F1-Score: 0.9861\n",
      "\n",
      " Average Cross-Validation Scores \n",
      "Avg Accuracy: 0.9758\n",
      "Avg Precision: 0.9766\n",
      "Avg Recall: 0.9747\n",
      "Avg F1-Score: 0.9754\n",
      "\n",
      " Final Performance on Hold-Out Test Set \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      0.98      0.99       102\n",
      "entertainment       0.99      1.00      0.99        77\n",
      "     politics       0.98      0.98      0.98        84\n",
      "        sport       0.99      0.99      0.99       102\n",
      "         tech       0.99      1.00      0.99        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Evaluating Support Vector Machine (SVM) \")\n",
    "svm_classifier = SVC(random_state=42)\n",
    "svm_avg_scores = evaluate_model(svm_classifier, X_train, y_train, cv_strategy)\n",
    "\n",
    "print(\"\\n Final Performance on Hold-Out Test Set \")\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03c413",
   "metadata": {},
   "source": [
    "8. Comparison and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e881de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Side-by-Side Model Comparison (Average CV Scores) \n",
      "                     Avg Accuracy  Avg Precision  Avg Recall  Avg F1-Score\n",
      "Logistic Regression      0.974157       0.975409    0.972905      0.973940\n",
      "SVM                      0.975843       0.976554    0.974666      0.975386\n",
      "\n",
      "Discussion \n",
      "  Based on the average F1-score, the Support Vector Machine (SVM) performed slightly better.\n",
      "  Both models achieved very high performance (typically >95% accuracy), indicating that the BBC news dataset is well-separated and that TF-IDF features are very effective for this topic classification task.\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame(\n",
    "    [lr_avg_scores, svm_avg_scores],\n",
    "    index=[\"Logistic Regression\", \"SVM\"]\n",
    ")\n",
    "\n",
    "print(\"\\n Side-by-Side Model Comparison (Average CV Scores) \")\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nDiscussion \")\n",
    "if lr_avg_scores['Avg F1-Score'] > svm_avg_scores['Avg F1-Score']:\n",
    "    print(\"  Based on the average F1-score, Logistic Regression performed slightly better.\")\n",
    "elif svm_avg_scores['Avg F1-Score'] > lr_avg_scores['Avg F1-Score']:\n",
    "    print(\"  Based on the average F1-score, the Support Vector Machine (SVM) performed slightly better.\")\n",
    "else:\n",
    "    print(\"  Both models performed equally well based on the average F1-score.\")\n",
    "\n",
    "print(\"  Both models achieved very high performance (typically >95% accuracy), \"\n",
    "      \"indicating that the BBC news dataset is well-separated and that TF-IDF \"\n",
    "      \"features are very effective for this topic classification task.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19df89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
