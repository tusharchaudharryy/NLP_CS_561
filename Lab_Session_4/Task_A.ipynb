{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94dfc075",
   "metadata": {},
   "source": [
    "# Lab Session 4 - Task A: VSM for Word Similarity\n",
    "Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c35922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Setup Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7069b4",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb271f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found categories: ['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading categories: 100%|██████████| 5/5 [00:21<00:00,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded and processed 2225 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "\n",
    "def load_data(folder_path):\n",
    "    \"\"\"Loads and preprocesses all documents from the dataset folder.\"\"\"\n",
    "    all_docs = []\n",
    "    categories = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    print(f\"Found categories: {categories}\")\n",
    "    \n",
    "    for category in tqdm(categories, desc=\"Loading categories\"):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "        for filename in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    processed_doc = preprocess_text(text)\n",
    "                    if processed_doc:\n",
    "                        all_docs.append(processed_doc)\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read {file_path}: {e}\")\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "BBC_FOLDER = 'bbc'\n",
    "documents = load_data(BBC_FOLDER)\n",
    "print(f\"\\nLoaded and processed {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188163c9",
   "metadata": {},
   "source": [
    "Build Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a468da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Co-occurrence Matrix: 100%|██████████| 2225/2225 [00:17<00:00, 123.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "WINDOW_SIZE = 5  \n",
    "\n",
    "all_tokens = [token for doc in documents for token in doc]\n",
    "word_counts = Counter(all_tokens)\n",
    "vocab = [word for word, count in word_counts.most_common(VOCAB_SIZE)]\n",
    "\n",
    "word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "id_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "vocab_set = set(vocab)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "cooc_matrix = lil_matrix((VOCAB_SIZE, VOCAB_SIZE), dtype=np.float32)\n",
    "\n",
    "for doc in tqdm(documents, desc=\"Building Co-occurrence Matrix\"):\n",
    "    doc_indices = [word_to_id[word] for word in doc if word in vocab_set]\n",
    "    for i, target_idx in enumerate(doc_indices):\n",
    "        start = max(0, i - WINDOW_SIZE)\n",
    "        end = min(len(doc_indices), i + WINDOW_SIZE + 1)\n",
    "        context_indices = doc_indices[start:i] + doc_indices[i+1:end]\n",
    "        for context_idx in context_indices:\n",
    "            cooc_matrix[target_idx, context_idx] += 1\n",
    "\n",
    "print(\"Co-occurrence matrix built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58f7d9",
   "metadata": {},
   "source": [
    "Apply PPMI Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781fa60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PPMI: 100%|██████████| 2416672/2416672 [00:48<00:00, 49548.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task A.a Result ---\n",
      "The dimensions of the final PPMI matrix are: (10000, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_ppmi(matrix):\n",
    "    matrix_csr = matrix.tocsr()\n",
    "    \n",
    "    total_cooccurrences = matrix_csr.sum()\n",
    "    if total_cooccurrences == 0:\n",
    "        return lil_matrix(matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    word_totals = np.array(matrix_csr.sum(axis=1)).flatten()\n",
    "    context_totals = np.array(matrix_csr.sum(axis=0)).flatten()\n",
    "    \n",
    "    p_wc = matrix_csr / total_cooccurrences\n",
    "    p_w = word_totals / total_cooccurrences\n",
    "    p_c = context_totals / total_cooccurrences\n",
    "\n",
    "    ppmi_matrix = lil_matrix(matrix.shape, dtype=np.float32)\n",
    "    rows, cols = matrix_csr.nonzero()\n",
    "    \n",
    "    for r, c in tqdm(zip(rows, cols), desc=\"Calculating PPMI\", total=len(rows)):\n",
    "        if p_w[r] > 0 and p_c[c] > 0:\n",
    "            pmi = np.log2(p_wc[r, c] / (p_w[r] * p_c[c]))\n",
    "            ppmi_matrix[r, c] = max(0, pmi)\n",
    "            \n",
    "    return ppmi_matrix\n",
    "\n",
    "\n",
    "ppmi_matrix = calculate_ppmi(cooc_matrix)\n",
    "print(\"\\n--- Task A.a Result ---\")\n",
    "print(f\"The dimensions of the final PPMI matrix are: {ppmi_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa67c0",
   "metadata": {},
   "source": [
    "Apply Truncated SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12297fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD matrix created with dimensions: (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "SVD_DIMENSIONS = 300\n",
    "\n",
    "svd = TruncatedSVD(n_components=SVD_DIMENSIONS, random_state=42)\n",
    "svd_matrix = svd.fit_transform(ppmi_matrix)\n",
    "\n",
    "print(f\"SVD matrix created with dimensions: {svd_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d35da3",
   "metadata": {},
   "source": [
    "Word Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa0e7813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Similarity Results:\n",
      "          Query Word                      Top 5 Similar Words (VSM)                    Top 5 Similar Words (SVD)\n",
      "   market (business)          stock, housing, growth, prices, sales       stock, analysts, share, growth, prices\n",
      "film (entertainment)         best, awards, actress, director, films          films, movie, awards, best, actress\n",
      " election (politics)        general, labour, campaign, blair, party   labour, general, partys, campaign, labours\n",
      "        game (sport)           games, play, players, match, playing         play, games, players, playing, first\n",
      "     software (tech) microsoft, programs, users, antivirus, windows programs, microsoft, windows, users, program\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar(query_word, matrix, word_to_id, id_to_word, top_n=5):\n",
    "    \"\"\"Finds the top_n most similar words to a query_word in a given matrix.\"\"\"\n",
    "    if query_word not in word_to_id:\n",
    "        return [(\"Word not in vocabulary\", 0)] * top_n\n",
    "    \n",
    "    query_id = word_to_id[query_word]\n",
    "    query_vector = matrix[query_id].reshape(1, -1)\n",
    "    \n",
    "    sim_scores = cosine_similarity(query_vector, matrix).flatten()\n",
    "    \n",
    "    top_indices = np.argsort(sim_scores)[::-1][1:top_n+1]\n",
    "    \n",
    "    return [(id_to_word[i], sim_scores[i]) for i in top_indices]\n",
    "\n",
    "\n",
    "\n",
    "query_words = {\n",
    "    'business': 'market',\n",
    "    'entertainment': 'film',\n",
    "    'politics': 'election',\n",
    "    'sport': 'game',\n",
    "    'tech': 'software'\n",
    "}\n",
    "\n",
    "results = []\n",
    "for category, word in query_words.items():\n",
    "    vsm_similar = find_most_similar(word, ppmi_matrix, word_to_id, id_to_word)\n",
    "    svd_similar = find_most_similar(word, svd_matrix, word_to_id, id_to_word)\n",
    "    \n",
    "    results.append({\n",
    "        \"Query Word\": f\"{word} ({category})\",\n",
    "        \"Top 5 Similar Words (VSM)\": \", \".join([f\"{w[0]}\" for w in vsm_similar]),\n",
    "        \"Top 5 Similar Words (SVD)\": \", \".join([f\"{w[0]}\" for w in svd_similar])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nWord Similarity Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f725b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab 4 results saved to task_a_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Run this in your Lab 4, Task A notebook\n",
    "results_df.to_csv('task_a_results.csv', index=False)\n",
    "print(\"Lab 4 results saved to task_a_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc3b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
