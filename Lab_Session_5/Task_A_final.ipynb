{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67101217",
   "metadata": {},
   "source": [
    "# Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be74d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tushar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tushar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.sparse import dok_matrix, lil_matrix, save_npz, load_npz\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "DATA_PATH = \"bbc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cf423",
   "metadata": {},
   "source": [
    "Load and Preprocess (tokenize) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c02ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2225 documents across 5 categories.\n",
      "Vocab size: 27206\n"
     ]
    }
   ],
   "source": [
    "def load_bbc_dataset(base_path=DATA_PATH):\n",
    "    docs, labels = [], []\n",
    "    for category in os.listdir(base_path):\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for fname in os.listdir(category_path):\n",
    "                fpath = os.path.join(category_path, fname)\n",
    "                with open(fpath, \"r\", encoding=\"latin-1\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    docs.append(text)\n",
    "                    labels.append(category)\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_bbc_dataset()\n",
    "print(f\"Loaded {len(docs)} documents across {len(set(labels))} categories.\")\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokenized_docs = [preprocess(doc) for doc in docs]\n",
    "\n",
    "vocab = sorted(set([w for doc in tokenized_docs for w in doc]))\n",
    "\n",
    "window_size = 5\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "V = len(vocab)\n",
    "print(f\"Vocab size: {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ead35",
   "metadata": {},
   "source": [
    "Compute and Maintain PPMI Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "936575a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPMI matrix from disk: (27206, 27206)\n"
     ]
    }
   ],
   "source": [
    "ppmi_file = \"ppmi_matrix.npz\"\n",
    "\n",
    "def compute_ppmi_mat () :\n",
    "    cooc_mat = dok_matrix((V, V), dtype=np.float32)\n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            if word not in word2idx: \n",
    "                continue\n",
    "            w_idx = word2idx[word]\n",
    "            context = doc[max(0, i - window_size): i] + doc[i+1: i+1+window_size]\n",
    "            for c in context:\n",
    "                if c in word2idx:\n",
    "                    c_idx = word2idx[c]\n",
    "                    cooc_mat[w_idx, c_idx] += 1\n",
    "\n",
    "    cooc_mat = cooc_mat.tocsr()\n",
    "\n",
    "    ppmi_mat = lil_matrix(cooc_mat.shape, dtype=np.float32)\n",
    "    total_count = cooc_mat.sum()\n",
    "    word_freq = np.array(cooc_mat.sum(axis=1)).flatten()\n",
    "    context_freq = np.array(cooc_mat.sum(axis=0)).flatten()\n",
    "\n",
    "    rows, cols = cooc_mat.nonzero()\n",
    "    for i, j in zip(rows, cols):\n",
    "        val = cooc_mat[i, j]\n",
    "        p_ij = val / total_count\n",
    "        p_i = word_freq[i] / total_count\n",
    "        p_j = context_freq[j] / total_count\n",
    "        score = np.log2(p_ij / (p_i * p_j))\n",
    "        if score > 0:\n",
    "            ppmi_mat[i, j] = score\n",
    "\n",
    "    ppmi_mat = ppmi_mat.tocsr()\n",
    "    return ppmi_mat\n",
    "\n",
    "if os.path.exists(ppmi_file):\n",
    "    ppmi_mat = load_npz(ppmi_file)\n",
    "    print(\"Loaded PPMI matrix from disk:\", ppmi_mat.shape)\n",
    "else:\n",
    "    ppmi_mat = compute_ppmi_mat()\n",
    "    save_npz(ppmi_file, ppmi_mat)\n",
    "    print(\"Computed and saved PPMI matrix:\", ppmi_mat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd44f8e",
   "metadata": {},
   "source": [
    "Compute and Maintain SVD Reduced Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f0b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SVD-reduced matrix from disk: (27206, 300)\n"
     ]
    }
   ],
   "source": [
    "svd_file = \"svd_matrix.npy\"\n",
    "def compute_svd_mat(d=300):\n",
    "    svd = TruncatedSVD(n_components=d, random_state=42)\n",
    "    svd_mat = svd.fit_transform(ppmi_mat)\n",
    "    return svd_mat\n",
    "\n",
    "if os.path.exists(svd_file):\n",
    "    svd_mat = np.load(svd_file)\n",
    "    print(\"Loaded SVD-reduced matrix from disk:\", svd_mat.shape)\n",
    "else:\n",
    "    svd_mat = compute_svd_mat()\n",
    "    np.save(svd_file, svd_mat)\n",
    "    print(\"Computed and saved SVD-reduced matrix:\", svd_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2c711",
   "metadata": {},
   "source": [
    "Defining SkipGram Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76c47cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tensor = torch.tensor(svd_mat, dtype=torch.float32)\n",
    "vocab_size, embedding_dim = svd_tensor.shape\n",
    "num_negative_samples = 5  \n",
    "\n",
    "# Create unigram distribution raised to 3/4 for negative sampling\n",
    "word_freqs = np.array([np.sum(ppmi_mat.getrow(i)) for i in range(vocab_size)])\n",
    "unigram_dist = word_freqs ** 0.75\n",
    "unigram_dist = unigram_dist / unigram_dist.sum()\n",
    "\n",
    "# Convert to torch tensor for efficient sampling\n",
    "unigram_dist = torch.tensor(unigram_dist)\n",
    "\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, svd_tensor):\n",
    "        super().__init__()\n",
    "        self.in_embeddings = nn.Embedding.from_pretrained(svd_tensor.clone(), freeze=False)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.uniform_(self.out_embeddings.weight, -0.5/embedding_dim, 0.5/embedding_dim)\n",
    "\n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        center_embeds = self.in_embeddings(center_words)  # (batch_size, embed_dim)\n",
    "        pos_embeds = self.out_embeddings(pos_context_words)  # (batch_size, embed_dim)\n",
    "        neg_embeds = self.out_embeddings(neg_context_words)  # (batch_size, num_neg_samples, embed_dim)\n",
    "\n",
    "        pos_score = torch.sum(center_embeds * pos_embeds, dim=1)  # (batch_size)\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-10)  # avoid log(0)\n",
    "\n",
    "        neg_score = torch.bmm(neg_embeds.neg(), center_embeds.unsqueeze(2)).squeeze()  # (batch_size, num_neg_samples)\n",
    "        neg_loss = torch.log(torch.sigmoid(neg_score) + 1e-10).sum(1)  # (batch_size)\n",
    "\n",
    "        loss = - (pos_loss + neg_loss).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "808c5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_samples(batch_size, num_neg_samples, unigram_dist):\n",
    "    neg_samples = torch.multinomial(unigram_dist, batch_size * num_neg_samples, replacement=True)\n",
    "    neg_samples = neg_samples.view(batch_size, num_neg_samples)\n",
    "    return neg_samples\n",
    "\n",
    "def generate_training_data(tokenized_docs, window_size=5, neg_samples=5):\n",
    "    pairs = []\n",
    "    for doc in tokenized_docs:\n",
    "        idxs = [word2idx[w] for w in doc if w in word2idx]\n",
    "        for i, w in enumerate(idxs):\n",
    "            context = idxs[max(0,i-window_size):i] + idxs[i+1:i+1+window_size+1]\n",
    "            for c in context:\n",
    "                pairs.append((w, c))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0638d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 5069429\n",
      "Resuming training from epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar\\AppData\\Local\\Temp\\ipykernel_11292\\752862063.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SkipGramNegSampling(vocab_size, embedding_dim, svd_tensor)\n",
    "model.to(device)  # Move model to GPU or CPU\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "training_pairs = generate_training_data(tokenized_docs)\n",
    "print(f\"Training pairs: {len(training_pairs)}\")\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    model.train()\n",
    "    return epoch, loss\n",
    "\n",
    "def find_last_checkpoint(directory):\n",
    "    checkpoints = [f for f in os.listdir(directory) if f.endswith('.pt')]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    epochs = [int(f.split('_')[1].split('.pt')[0]) for f in checkpoints if '_' in f]\n",
    "    if not epochs:\n",
    "        return None\n",
    "    max_epoch = max(epochs)\n",
    "    return os.path.join(directory, f\"checkpoint_{max_epoch}.pt\"), max_epoch\n",
    "\n",
    "start_epoch = 0\n",
    "training_epochs = 20\n",
    "\n",
    "last_ckpt = find_last_checkpoint(checkpoint_dir)\n",
    "if last_ckpt is not None:\n",
    "    ckpt_path, start_epoch = last_ckpt\n",
    "    start_epoch, val_loss = load_checkpoint(ckpt_path, model, optimizer)\n",
    "    start_epoch += 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "batch_size = 128\n",
    "neg_samples = 5\n",
    "\n",
    "for epoch in range(start_epoch, training_epochs):\n",
    "    total_loss = 0\n",
    "    np.random.shuffle(training_pairs)\n",
    "\n",
    "    for i in range(0, min(len(training_pairs), 100000), batch_size):\n",
    "        batch = training_pairs[i:i+batch_size]\n",
    "\n",
    "        center_batch = torch.tensor([c for c, _ in batch], dtype=torch.long).to(device)\n",
    "        context_batch = torch.tensor([ctx for _, ctx in batch], dtype=torch.long).to(device)\n",
    "        neg_batch = torch.tensor(np.random.choice(len(vocab), size=(len(batch), neg_samples), replace=True), dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center_batch, context_batch, neg_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(batch)\n",
    "\n",
    "    avg_loss = total_loss / min(len(training_pairs), 100000)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "    save_checkpoint(model, optimizer, epoch, avg_loss, ckpt_path)\n",
    "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "w2v_embeddings = model.in_embeddings.weight.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f037a",
   "metadata": {},
   "source": [
    "## Eval 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "639933ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>VSM</th>\n",
       "      <th>SVD</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>market</td>\n",
       "      <td>[stock, share, housing, prices, growth]</td>\n",
       "      <td>[stock, share, analysts, housing, prices]</td>\n",
       "      <td>[stock, share, prices, exchange, analysts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film</td>\n",
       "      <td>[festival, directed, director, awards, films]</td>\n",
       "      <td>[directed, festival, movie, films, fuqua]</td>\n",
       "      <td>[festival, directed, movie, films, hollywood]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>election</td>\n",
       "      <td>[general, labour, campaign, party, tories]</td>\n",
       "      <td>[labour, general, campaign, tories, party]</td>\n",
       "      <td>[labour, general, campaign, campaigning, tories]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>football</td>\n",
       "      <td>[league, club, manager, manchester, ferguson]</td>\n",
       "      <td>[club, warding, league, docherty, clubs]</td>\n",
       "      <td>[club, league, champions, coach, manchester]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>[software, users, system, mac, pc]</td>\n",
       "      <td>[pc, software, computers, frees, docking]</td>\n",
       "      <td>[software, pc, computers, use, means]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Query                                            VSM  \\\n",
       "0    market        [stock, share, housing, prices, growth]   \n",
       "1      film  [festival, directed, director, awards, films]   \n",
       "2  election     [general, labour, campaign, party, tories]   \n",
       "3  football  [league, club, manager, manchester, ferguson]   \n",
       "4  computer             [software, users, system, mac, pc]   \n",
       "\n",
       "                                          SVD  \\\n",
       "0   [stock, share, analysts, housing, prices]   \n",
       "1   [directed, festival, movie, films, fuqua]   \n",
       "2  [labour, general, campaign, tories, party]   \n",
       "3    [club, warding, league, docherty, clubs]   \n",
       "4   [pc, software, computers, frees, docking]   \n",
       "\n",
       "                                           Word2Vec  \n",
       "0        [stock, share, prices, exchange, analysts]  \n",
       "1     [festival, directed, movie, films, hollywood]  \n",
       "2  [labour, general, campaign, campaigning, tories]  \n",
       "3      [club, league, champions, coach, manchester]  \n",
       "4             [software, pc, computers, use, means]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def top_k_similar(word, mat, k=5):\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    idx = word2idx[word]\n",
    "\n",
    "    if hasattr(mat, \"tocsr\"):  # sparse case\n",
    "        vec = mat.getrow(idx)        # 1 x V\n",
    "        sims = cosine_similarity(vec, mat)[0]\n",
    "    else:  # dense numpy\n",
    "        sims = cosine_similarity([mat[idx]], mat)[0]\n",
    "\n",
    "    top_idx = sims.argsort()[::-1][1:k+1]\n",
    "    return [idx2word[i] for i in top_idx]\n",
    "\n",
    "query_words = [\"market\", \"film\", \"election\", \"football\", \"computer\"]\n",
    "\n",
    "results = []\n",
    "for q in query_words:\n",
    "    vsm_res = top_k_similar(q, ppmi_mat)\n",
    "    svd_res = top_k_similar(q, svd_mat)\n",
    "    w2v_res = top_k_similar(q, w2v_embeddings)\n",
    "    results.append([q, vsm_res, svd_res, w2v_res])\n",
    "\n",
    "pd.DataFrame(results, columns=[\"Query\", \"VSM\", \"SVD\", \"Word2Vec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3151853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business is to profit as politics is to would\n",
      "britain is to london as france is to germany\n",
      "sport is to football as tech is to explanations\n",
      "minister is to government as player is to prime\n",
      "movie is to entertainment as legislation is to starring\n"
     ]
    }
   ],
   "source": [
    "def analogy(a, b, c, embeddings, k=1):\n",
    "    if any(w not in word2idx for w in [a,b,c]):\n",
    "        return None\n",
    "    vec = embeddings[word2idx[a]] - embeddings[word2idx[b]] + embeddings[word2idx[c]]\n",
    "    sims = cosine_similarity([vec], embeddings)[0]\n",
    "    best = sims.argsort()[::-1]\n",
    "    for idx in best:\n",
    "        word = idx2word[idx]\n",
    "        if word not in [a,b,c]:\n",
    "            return word\n",
    "\n",
    "questions = [\n",
    "    (\"business\", \"profit\", \"politics\"),\n",
    "    (\"britain\", \"london\", \"france\"),\n",
    "    (\"sport\", \"football\", \"tech\"),\n",
    "    (\"minister\", \"government\", \"player\"),\n",
    "    (\"movie\", \"entertainment\", \"legislation\")\n",
    "\n",
    "]\n",
    "\n",
    "for a,b,c in questions:\n",
    "    print(f\"{a} is to {b} as {c} is to {analogy(a,b,c,w2v_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16dc7dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>VSM</th>\n",
       "      <th>SVD</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mirza</td>\n",
       "      <td>[sania, bondarenko, jelena, jankovic, alyona]</td>\n",
       "      <td>[sania, hyderabad, bondarenko, jelena, jankovic]</td>\n",
       "      <td>[sania, serena, hyderabad, round, martinez]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query                                            VSM  \\\n",
       "0  mirza  [sania, bondarenko, jelena, jankovic, alyona]   \n",
       "\n",
       "                                                SVD  \\\n",
       "0  [sania, hyderabad, bondarenko, jelena, jankovic]   \n",
       "\n",
       "                                      Word2Vec  \n",
       "0  [sania, serena, hyderabad, round, martinez]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_words = [\"mirza\"]\n",
    "\n",
    "results = []\n",
    "for q in query_words:\n",
    "    vsm_res = top_k_similar(q, ppmi_mat)\n",
    "    svd_res = top_k_similar(q, svd_mat)\n",
    "    w2v_res = top_k_similar(q, w2v_embeddings)\n",
    "    results.append([q, vsm_res, svd_res, w2v_res])\n",
    "\n",
    "pd.DataFrame(results, columns=[\"Query\", \"VSM\", \"SVD\", \"Word2Vec\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
